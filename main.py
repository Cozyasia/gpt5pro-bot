# -*- coding: utf-8 -*-
import os
import re
import json
import base64
import logging
from io import BytesIO

from telegram import Update
from telegram.ext import (
    ApplicationBuilder, CommandHandler, MessageHandler, ContextTypes, filters
)
from telegram.constants import ChatAction

# ========== LOGGING ==========
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s | %(levelname)s | %(name)s | %(message)s",
)
log = logging.getLogger("gpt-bot")

# ========== ENV ==========
BOT_TOKEN       = os.environ.get("BOT_TOKEN", "").strip()
PUBLIC_URL      = os.environ.get("PUBLIC_URL", "").strip()   # https://<subdomain>.onrender.com
OPENAI_API_KEY  = os.environ.get("OPENAI_API_KEY", "").strip()
OPENAI_MODEL    = os.environ.get("OPENAI_MODEL", "gpt-4o-mini").strip()
WEBHOOK_SECRET  = os.environ.get("WEBHOOK_SECRET", "").strip()
BANNER_URL      = os.environ.get("BANNER_URL", "").strip()   # Ð¼Ð¾Ð¶Ð½Ð¾ Ð¿ÑƒÑÑ‚Ñ‹Ð¼
TAVILY_API_KEY  = os.environ.get("TAVILY_API_KEY", "").strip()
TRANSCRIBE_MODEL = os.environ.get("OPENAI_TRANSCRIBE_MODEL", "whisper-1").strip()
PORT            = int(os.environ.get("PORT", "10000"))

if not BOT_TOKEN:
    raise RuntimeError("ENV BOT_TOKEN is required")
if not PUBLIC_URL or not PUBLIC_URL.startswith("http"):
    raise RuntimeError("ENV PUBLIC_URL must look like https://xxx.onrender.com")
if not OPENAI_API_KEY:
    log.warning("OPENAI_API_KEY is empty â€” Ð¾Ñ‚Ð²ÐµÑ‚Ñ‹ Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ñ‚ÑŒ Ð½Ðµ Ð±ÑƒÐ´ÑƒÑ‚")

# ========== OPENAI / Tavily clients ==========
from openai import OpenAI
oai = OpenAI(api_key=OPENAI_API_KEY) if OPENAI_API_KEY else None

try:
    if TAVILY_API_KEY:
        from tavily import TavilyClient
        tavily = TavilyClient(api_key=TAVILY_API_KEY)
    else:
        tavily = None
except Exception:
    tavily = None

# ========== PROMPTS & HEURISTICS ==========
SYSTEM_PROMPT = (
    "Ð¢Ñ‹ Ð´Ñ€ÑƒÐ¶ÐµÐ»ÑŽÐ±Ð½Ñ‹Ð¹ Ð¸ Ð»Ð°ÐºÐ¾Ð½Ð¸Ñ‡Ð½Ñ‹Ð¹ Ð°ÑÑÐ¸ÑÑ‚ÐµÐ½Ñ‚ Ð½Ð° Ñ€ÑƒÑÑÐºÐ¾Ð¼. "
    "ÐžÑ‚Ð²ÐµÑ‡Ð°Ð¹ Ð¿Ð¾ ÑÑƒÑ‚Ð¸, Ð´Ð¾Ð±Ð°Ð²Ð»ÑÐ¹ ÑÐ¿Ð¸ÑÐºÐ¸ Ð¸ ÑˆÐ°Ð³Ð¸, ÐºÐ¾Ð³Ð´Ð° ÑÑ‚Ð¾ Ð¿Ð¾Ð»ÐµÐ·Ð½Ð¾. "
    "Ð•ÑÐ»Ð¸ Ð¿Ñ€Ð¸Ð²Ð¾Ð´Ð¸ÑˆÑŒ Ð¸ÑÑ‚Ð¾Ñ‡Ð½Ð¸ÐºÐ¸ â€” Ð² ÐºÐ¾Ð½Ñ†Ðµ Ð´Ð°Ð¹ ÐºÐ¾Ñ€Ð¾Ñ‚ÐºÐ¸Ð¹ ÑÐ¿Ð¸ÑÐ¾Ðº ÑÑÑ‹Ð»Ð¾Ðº. "
    "ÐÐµ Ð²Ñ‹Ð´ÑƒÐ¼Ñ‹Ð²Ð°Ð¹ Ñ„Ð°ÐºÑ‚Ñ‹; ÐµÑÐ»Ð¸ Ð½Ðµ ÑƒÐ²ÐµÑ€ÐµÐ½ â€” ÑÐºÐ°Ð¶Ð¸ Ð¾Ð± ÑÑ‚Ð¾Ð¼."
)

VISION_SYSTEM_PROMPT = (
    "Ð¢Ñ‹ Ð¾Ð¿Ð¸ÑÑ‹Ð²Ð°ÐµÑˆÑŒ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ñ: Ð¿Ñ€ÐµÐ´Ð¼ÐµÑ‚Ñ‹, Ñ‚ÐµÐºÑÑ‚, Ð¼Ð°ÐºÐµÑ‚Ñ‹, Ð³Ñ€Ð°Ñ„Ð¸ÐºÐ¸. "
    "ÐÐµ Ð¾Ð¿Ñ€ÐµÐ´ÐµÐ»ÑÐ¹ Ð»Ð¸Ñ‡Ð½Ð¾ÑÑ‚Ð¸ Ð»ÑŽÐ´ÐµÐ¹ Ð¸ Ð½Ðµ Ð´Ð°Ð²Ð°Ð¹ Ð¸Ñ… Ð¸Ð¼ÐµÐ½, ÐµÑÐ»Ð¸ Ð¾Ð½Ð¸ Ð½Ðµ Ð½Ð°Ð¿ÐµÑ‡Ð°Ñ‚Ð°Ð½Ñ‹ Ð½Ð° Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ð¸. "
    "Ð‘ÑƒÐ´ÑŒ ÐºÐ¾Ð½ÐºÑ€ÐµÑ‚Ð½Ñ‹Ð¼ Ð¸ Ð¿Ð¾Ð»ÐµÐ·Ð½Ñ‹Ð¼."
)

VISION_CAPABILITY_HELP = (
    "Ð”Ð° â€” Ñ ÑƒÐ¼ÐµÑŽ Ð°Ð½Ð°Ð»Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ñ. ÐŸÑ€Ð¸ÐºÑ€ÐµÐ¿Ð¸ Ñ„Ð¾Ñ‚Ð¾ Ð¸Ð»Ð¸ ÑÐºÑ€Ð¸Ð½ÑˆÐ¾Ñ‚ ðŸ“Ž\n"
    "â€¢ Ð¤Ð¾Ñ€Ð¼Ð°Ñ‚Ñ‹: JPG/PNG/WebP, Ð´Ð¾ ~10 ÐœÐ‘.\n"
    "â€¢ PDF Ð¸ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ñ‹ â€” Ð¿Ñ€Ð¸ÑˆÐ»Ð¸ ÐºÐ°Ðº *Ñ„Ð°Ð¹Ð»*, Ð¸Ð·Ð²Ð»ÐµÐºÑƒ Ñ‚ÐµÐºÑÑ‚/Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ñ‹.\n"
    "â€¢ Ð’Ð¸Ð´ÐµÐ¾: Ð¿Ñ€Ð¸ÑˆÐ»Ð¸ 1â€“3 ÑÐºÑ€Ð¸Ð½ÑˆÐ¾Ñ‚Ð° (ÐºÐ°Ð´Ñ€Ð°) â€” Ð¾Ð¿Ð¸ÑˆÑƒ Ð¸ Ð¿Ñ€Ð¾Ð°Ð½Ð°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÑŽ Ð¿Ð¾ ÐºÐ°Ð´Ñ€Ð°Ð¼.\n"
    "Ð•ÑÐ»Ð¸ Ñ„Ð°Ð¹Ð» ÑƒÐ¶Ðµ Ð¾Ñ‚Ð¿Ñ€Ð°Ð²Ð»ÐµÐ½ â€” Ð¿Ñ€Ð¾ÑÑ‚Ð¾ Ð´Ð¾Ð±Ð°Ð²ÑŒ Ð²Ð¾Ð¿Ñ€Ð¾Ñ Ðº Ð½ÐµÐ¼Ñƒ ðŸ˜‰"
)

_SMALLTALK_RE = re.compile(
    r"^(Ð¿Ñ€Ð¸Ð²ÐµÑ‚|Ð·Ð´Ñ€Ð°Ð²ÑÑ‚Ð²ÑƒÐ¹|Ð´Ð¾Ð±Ñ€Ñ‹Ð¹\s*(Ð´ÐµÐ½ÑŒ|Ð²ÐµÑ‡ÐµÑ€|ÑƒÑ‚Ñ€Ð¾)|Ñ…Ð¸|hi|hello|Ñ…ÐµÐ»Ð»Ð¾|ÐºÐ°Ðº Ð´ÐµÐ»Ð°|ÑÐ¿Ð°ÑÐ¸Ð±Ð¾|Ð¿Ð¾ÐºÐ°)\b",
    re.IGNORECASE
)
_NEWSY_RE = re.compile(
    r"(ÐºÐ¾Ð³Ð´Ð°|Ð´Ð°Ñ‚Ð°|Ð²Ñ‹Ð¹Ð´ÐµÑ‚|Ñ€ÐµÐ»Ð¸Ð·|Ð½Ð¾Ð²Ð¾ÑÑ‚|ÐºÑƒÑ€Ñ|Ñ†ÐµÐ½Ð°|Ð¿Ñ€Ð¾Ð³Ð½Ð¾Ð·|Ñ‡Ñ‚Ð¾ Ñ‚Ð°ÐºÐ¾Ðµ|ÐºÑ‚Ð¾ Ñ‚Ð°ÐºÐ¾Ð¹|Ð½Ð°Ð¹Ð´Ð¸|ÑÑÑ‹Ð»ÐºÐ°|Ð¾Ñ„Ð¸Ñ†Ð¸Ð°Ð»|Ð°Ð´Ñ€ÐµÑ|Ñ‚ÐµÐ»ÐµÑ„Ð¾Ð½|"
    r"Ð¿Ð¾Ð³Ð¾Ð´Ð°|ÑÐµÐ³Ð¾Ð´Ð½Ñ|ÑÐµÐ¹Ñ‡Ð°Ñ|ÑˆÑ‚Ñ€Ð°Ñ„|Ð·Ð°ÐºÐ¾Ð½|Ñ‚Ñ€ÐµÐ½Ð´|ÐºÐ¾Ñ‚Ð¸Ñ€Ð¾Ð²Ðº|Ð¾Ð±Ð·Ð¾Ñ€|Ñ€Ð°ÑÐ¿Ð¸ÑÐ°Ð½Ð¸|Ð·Ð°Ð¿ÑƒÑÐº|update|Ð½Ð¾Ð²Ð°Ñ Ð²ÐµÑ€ÑÐ¸Ñ)",
    re.IGNORECASE
)
_CAPABILITY_RE = re.compile(
    r"(Ð¼Ð¾Ð¶(ÐµÑˆÑŒ|Ð½Ð¾)\s*(Ð»Ð¸\s*)?(Ð°Ð½Ð°Ð»Ð¸Ð·(Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ)?|Ñ€Ð°ÑÐ¿Ð¾Ð·Ð½Ð°Ð²(Ð°Ñ‚ÑŒ|Ð°Ð½Ð¸Ðµ))\s*(Ñ„Ð¾Ñ‚Ð¾|ÐºÐ°Ñ€Ñ‚Ð¸Ð½Ðº|Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½|image|picture)|"
    r"Ð°Ð½Ð°Ð»Ð¸Ð·(Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ)?\s*(Ñ„Ð¾Ñ‚Ð¾|ÐºÐ°Ñ€Ñ‚Ð¸Ð½Ðº|Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½)|"
    r"(Ð¼Ð¾Ð¶(ÐµÑˆÑŒ|Ð½Ð¾)\s*(Ð»Ð¸\s*)?)?(Ð°Ð½Ð°Ð»Ð¸Ð·|Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ñ‚ÑŒ)\s*Ñ\s*Ð²Ð¸Ð´ÐµÐ¾)",
    re.IGNORECASE
)

def is_smalltalk(text: str) -> bool:
    return bool(_SMALLTALK_RE.search(text.strip()))

def should_browse(text: str) -> bool:
    t = text.strip()
    if is_smalltalk(t):
        return False
    # ÐµÑÐ»Ð¸ ÑÐ²Ð½Ð°Ñ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ð¾Ð½Ð½Ð°Ñ Ñ†ÐµÐ»ÑŒ Ð¸Ð»Ð¸ Ð²Ð¾Ð¿Ñ€Ð¾Ñ â€“ ÑÐ¼Ð¾Ñ‚Ñ€Ð¸Ð¼ Ð² Ð¸Ð½Ñ‚ÐµÑ€Ð½ÐµÑ‚
    if _NEWSY_RE.search(t) or "?" in t or len(t) > 80:
        return True
    return False

def is_vision_capability_question(text: str) -> bool:
    return bool(_CAPABILITY_RE.search(text))

# ========== UTILS ==========
async def typing(ctx: ContextTypes.DEFAULT_TYPE, chat_id: int):
    try:
        await ctx.bot.send_chat_action(chat_id, action=ChatAction.TYPING)
    except Exception:
        pass

def sniff_image_mime(data: bytes) -> str:
    if data.startswith(b"\xff\xd8"):
        return "image/jpeg"
    if data.startswith(b"\x89PNG"):
        return "image/png"
    if data[:4] == b"RIFF" and b"WEBP" in data[:16]:
        return "image/webp"
    return "image/jpeg"

def format_sources(items):
    if not items:
        return ""
    lines = []
    for i, it in enumerate(items, 1):
        title = it.get("title") or it.get("url") or "Ð˜ÑÑ‚Ð¾Ñ‡Ð½Ð¸Ðº"
        url = it.get("url") or ""
        lines.append(f"[{i}] {title} â€” {url}")
    return "\n\nÐ¡ÑÑ‹Ð»ÐºÐ¸:\n" + "\n".join(lines)

def tavily_search(query: str, max_results: int = 5):
    if not tavily:
        return None, []
    try:
        res = tavily.search(
            query=query,
            search_depth="advanced",
            max_results=max_results,
            include_answer=True,
            include_raw_content=False,
        )
        answer = res.get("answer") or ""
        results = res.get("results") or []
        return answer, results
    except Exception as e:
        log.exception("Tavily error: %s", e)
        return None, []

async def ask_openai_text(user_text: str, web_ctx: str = "") -> str:
    """Ð§Ð¸ÑÑ‚Ð¾ Ñ‚ÐµÐºÑÑ‚Ð¾Ð²Ñ‹Ð¹ Ð¾Ñ‚Ð²ÐµÑ‚ (Ñ Ð¾Ð¿Ñ†Ð¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ñ‹Ð¼ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ð¾Ð¼ ÑÑÑ‹Ð»Ð¾Ðº)."""
    if not oai:
        return "OPENAI_API_KEY Ð½Ðµ Ð·Ð°Ð´Ð°Ð½. Ð¡Ð¾Ð¾Ð±Ñ‰Ð¸ Ð°Ð´Ð¼Ð¸Ð½Ñƒ."
    messages = [{"role": "system", "content": SYSTEM_PROMPT}]
    if web_ctx:
        messages.append({"role": "system", "content": f"Ð’ Ð¿Ð¾Ð¼Ð¾Ñ‰ÑŒ Ñ‚ÐµÐ±Ðµ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚ Ñ Ð²ÐµÐ±-Ð¸ÑÑ‚Ð¾Ñ‡Ð½Ð¸ÐºÐ°Ð¼Ð¸:\n{web_ctx}"})
    messages.append({"role": "user", "content": user_text})
    try:
        resp = oai.chat.completions.create(
            model=OPENAI_MODEL,
            messages=messages,
            temperature=0.6,
        )
        return (resp.choices[0].message.content or "").strip()
    except Exception as e:
        log.exception("OpenAI chat error: %s", e)
        return "ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ð¿Ð¾Ð»ÑƒÑ‡Ð¸Ñ‚ÑŒ Ð¾Ñ‚Ð²ÐµÑ‚ Ð¾Ñ‚ Ð¼Ð¾Ð´ÐµÐ»Ð¸. ÐŸÐ¾Ð¿Ñ€Ð¾Ð±ÑƒÐ¹ ÐµÑ‰Ñ‘ Ñ€Ð°Ð· Ð¿Ð¾Ð·Ð¶Ðµ."

async def ask_openai_vision(user_text: str, img_b64: str, mime: str) -> str:
    """ÐÐ½Ð°Ð»Ð¸Ð· Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ñ + Ñ‚ÐµÐºÑÑ‚Ð¾Ð²Ñ‹Ð¹ Ð²Ð¾Ð¿Ñ€Ð¾Ñ."""
    if not oai:
        return "OPENAI_API_KEY Ð½Ðµ Ð·Ð°Ð´Ð°Ð½. Ð¡Ð¾Ð¾Ð±Ñ‰Ð¸ Ð°Ð´Ð¼Ð¸Ð½Ñƒ."
    try:
        resp = oai.chat.completions.create(
            model=OPENAI_MODEL,
            messages=[
                {"role": "system", "content": VISION_SYSTEM_PROMPT},
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": user_text or "ÐžÐ¿Ð¸ÑˆÐ¸, Ñ‡Ñ‚Ð¾ Ð½Ð° Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ð¸ Ð¸ ÐºÐ°ÐºÐ¾Ð¹ Ñ‚Ð°Ð¼ Ñ‚ÐµÐºÑÑ‚."},
                        {"type": "image_url",
                         "image_url": {"url": f"data:{mime};base64,{img_b64}"}}
                    ]
                }
            ],
            temperature=0.4,
        )
        return (resp.choices[0].message.content or "").strip()
    except Exception as e:
        log.exception("Vision error: %s", e)
        return "ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ð¿Ñ€Ð¾Ð°Ð½Ð°Ð»Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ðµ. ÐŸÐ¾Ð¿Ñ€Ð¾Ð±ÑƒÐ¹ ÐµÑ‰Ñ‘ Ñ€Ð°Ð· Ð¸Ð»Ð¸ Ð¿Ñ€Ð¸ÑˆÐ»Ð¸ Ð´Ñ€ÑƒÐ³Ð¾Ð¹ Ñ„Ð°Ð¹Ð»."

async def transcribe_audio(buf: BytesIO, filename_hint: str = "audio.ogg") -> str:
    """Ð Ð°ÑÐ¿Ð¾Ð·Ð½Ð°Ñ‘Ð¼ Ð³Ð¾Ð»Ð¾Ñ (OGG/OPUS Ð¸ Ð¿Ñ€.), Ð²Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÐ¼ Ñ‚ÐµÐºÑÑ‚."""
    if not oai:
        return ""
    try:
        # OpenAI SDK Ð¾Ð¶Ð¸Ð´Ð°ÐµÑ‚ file-like Ñ Ð¸Ð¼ÐµÐ½ÐµÐ¼; Ð¿Ñ€Ð¸ÑÐ²Ð¾Ð¸Ð¼ BytesIO Â«nameÂ»
        buf.seek(0)
        setattr(buf, "name", filename_hint)
        tr = oai.audio.transcriptions.create(
            model=TRANSCRIBE_MODEL,
            file=buf
        )
        text = (tr.text or "").strip()
        return text
    except Exception as e:
        log.exception("Transcribe error: %s", e)
        return ""

# ========== HANDLERS ==========
START_GREETING = (
    "ÐŸÑ€Ð¸Ð²ÐµÑ‚! Ð¯ Ð³Ð¾Ñ‚Ð¾Ð². ÐÐ°Ð¿Ð¸ÑˆÐ¸ Ð»ÑŽÐ±Ð¾Ð¹ Ð²Ð¾Ð¿Ñ€Ð¾Ñ.\n\n"
    "ÐŸÐ¾Ð´ÑÐºÐ°Ð·ÐºÐ¸:\n"
    "â€¢ Ð¯ Ð¸Ñ‰Ñƒ ÑÐ²ÐµÐ¶ÑƒÑŽ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸ÑŽ Ð² Ð¸Ð½Ñ‚ÐµÑ€Ð½ÐµÑ‚Ðµ Ð´Ð»Ñ Ñ„Ð°ÐºÑ‚Ð¾Ð² Ð¸ Ð´Ð°Ñ‚, ÐºÐ¾Ð³Ð´Ð° ÑÑ‚Ð¾ Ð½ÑƒÐ¶Ð½Ð¾.\n"
    "â€¢ ÐŸÑ€Ð¸Ð¼ÐµÑ€Ñ‹: Â«ÐšÐ¾Ð³Ð´Ð° Ð²Ñ‹Ð¹Ð´ÐµÑ‚ GTA 6?Â», Â«ÐšÑƒÑ€Ñ Ð±Ð¸Ñ‚ÐºÐ¾Ð¸Ð½Ð° ÑÐµÐ¹Ñ‡Ð°Ñ Ð¸ Ð¿Ñ€Ð¾Ð³Ð½Ð¾Ð·Â», "
    "Â«ÐÐ°Ð¹Ð´Ð¸ ÑƒÑ‡ÐµÐ±Ð½Ð¸Ðº Ð°Ð»Ð³ÐµÐ±Ñ€Ñ‹ 11 ÐºÐ»Ð°ÑÑ (Ð¾Ñ„Ð¸Ñ†Ð¸Ð°Ð»ÑŒÐ½Ñ‹Ðµ Ð¸ÑÑ‚Ð¾Ñ‡Ð½Ð¸ÐºÐ¸)Â», Â«ÐÐ¾Ð²Ð¾ÑÑ‚Ð¸ Ð¿Ð¾ ...?Â»\n"
    "â€¢ ÐœÐ¾Ð¶Ð½Ð¾ Ð¿Ñ€Ð¸ÑÐ»Ð°Ñ‚ÑŒ Ñ„Ð¾Ñ‚Ð¾ â€” Ð¾Ð¿Ð¸ÑˆÑƒ Ð¸ Ð¸Ð·Ð²Ð»ÐµÐºÑƒ Ñ‚ÐµÐºÑÑ‚.\n"
    "â€¢ ÐœÐ¾Ð¶Ð½Ð¾ Ð¾Ñ‚Ð¿Ñ€Ð°Ð²Ð¸Ñ‚ÑŒ Ð³Ð¾Ð»Ð¾ÑÐ¾Ð²Ð¾Ðµ â€” Ñ Ñ€Ð°ÑÐ¿Ð¾Ð·Ð½Ð°ÑŽ Ð¸ Ð¾Ñ‚Ð²ÐµÑ‡Ñƒ Ð¿Ð¾ ÑÐ¾Ð´ÐµÑ€Ð¶Ð°Ð½Ð¸ÑŽ."
)

async def cmd_start(update: Update, context: ContextTypes.DEFAULT_TYPE):
    # Ð±Ð°Ð½Ð½ÐµÑ€ â€” Ð¾Ð¿Ñ†Ð¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ð¾
    if BANNER_URL:
        try:
            await update.effective_message.reply_photo(BANNER_URL)
        except Exception:
            pass
    await update.effective_message.reply_text(START_GREETING)

async def on_text(update: Update, context: ContextTypes.DEFAULT_TYPE):
    text = (update.message.text or "").strip()
    chat_id = update.effective_chat.id

    # Ð’Ð¾Ð¿Ñ€Ð¾Ñ Ð¿Ñ€Ð¾ Ð²Ð¾Ð·Ð¼Ð¾Ð¶Ð½Ð¾ÑÑ‚Ð¸ Ð°Ð½Ð°Ð»Ð¸Ð·Ð° Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ð¹/Ð²Ð¸Ð´ÐµÐ¾
    if is_vision_capability_question(text):
        await update.message.reply_text(VISION_CAPABILITY_HELP, disable_web_page_preview=True)
        return

    await typing(context, chat_id)

    # ÐœÐ°Ð»ÐµÐ½ÑŒÐºÐ¸Ðµ Ñ€Ð°Ð·Ð³Ð¾Ð²Ð¾Ñ€Ð½Ñ‹Ðµ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ñ â€” Ð±ÐµÐ· Ð²ÐµÐ±Ð°
    if is_smalltalk(text):
        reply = await ask_openai_text(text)
        await update.message.reply_text(reply)
        return

    # ÐÑƒÐ¶ÐµÐ½ Ð»Ð¸ Ð²ÐµÐ±-Ð¿Ð¾Ð¸ÑÐº?
    web_ctx = ""
    sources = []
    if should_browse(text):
        answer_from_search, results = tavily_search(text, max_results=5)
        sources = results or []
        # ÐšÑ€Ð°Ñ‚ÐºÐ¸Ð¹ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚ Ð´Ð»Ñ Ð¼Ð¾Ð´ÐµÐ»Ð¸ (Ð¾Ñ‚Ð²ÐµÑ‚ + ÑÑÑ‹Ð»ÐºÐ¸)
        ctx_lines = []
        if answer_from_search:
            ctx_lines.append(f"ÐšÑ€Ð°Ñ‚ÐºÐ°Ñ ÑÐ²Ð¾Ð´ÐºÐ° Ð¿Ð¾Ð¸ÑÐºÐ¾Ð¼: {answer_from_search}")
        for i, it in enumerate(sources, 1):
            ctx_lines.append(f"[{i}] {it.get('title','')}: {it.get('url','')}")
        web_ctx = "\n".join(ctx_lines)

    # Ð“ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ Ð¾Ñ‚Ð²ÐµÑ‚Ð° Ð¼Ð¾Ð´ÐµÐ»Ð¸
    answer = await ask_openai_text(text, web_ctx=web_ctx)

    # ÐŸÑ€Ð¸ÐºÐ»ÐµÐ¸Ð¼ ÑÐ²Ð½Ñ‹Ðµ ÑÑÑ‹Ð»ÐºÐ¸ (ÐµÑÐ»Ð¸ Ð±Ñ‹Ð»Ð¸)
    answer += format_sources(sources)
    await update.message.reply_text(answer, disable_web_page_preview=False)

async def on_photo(update: Update, context: ContextTypes.DEFAULT_TYPE):
    chat_id = update.effective_chat.id
    await typing(context, chat_id)

    # Ð±ÐµÑ€Ñ‘Ð¼ ÑÐ°Ð¼Ñ‹Ð¹ ÐºÐ°Ñ‡ÐµÑÑ‚Ð²ÐµÐ½Ð½Ñ‹Ð¹ Ñ€Ð°Ð·Ð¼ÐµÑ€
    photo = update.message.photo[-1]
    file = await context.bot.get_file(photo.file_id)
    buf = BytesIO()
    await file.download_to_memory(buf)
    data = buf.getvalue()

    mime = sniff_image_mime(data)
    img_b64 = base64.b64encode(data).decode("ascii")

    # Ð¿Ð¾Ð¿Ñ€Ð¾Ð±ÑƒÐµÐ¼ Ð²Ð·ÑÑ‚ÑŒ Ð¿Ð¾Ð´Ð¿Ð¸ÑÑŒ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ ÐºÐ°Ðº Ð²Ð¾Ð¿Ñ€Ð¾Ñ
    user_text = (update.message.caption or "").strip()

    answer = await ask_openai_vision(user_text, img_b64, mime)
    await update.message.reply_text(answer, disable_web_page_preview=True)

async def on_voice(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """Voice message (OGG/OPUS)."""
    chat_id = update.effective_chat.id
    await typing(context, chat_id)

    voice = update.message.voice
    file = await context.bot.get_file(voice.file_id)
    buf = BytesIO()
    await file.download_to_memory(buf)

    text = await transcribe_audio(buf, filename_hint="audio.ogg")
    if not text:
        await update.message.reply_text("ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ñ€Ð°ÑÐ¿Ð¾Ð·Ð½Ð°Ñ‚ÑŒ Ð³Ð¾Ð»Ð¾Ñ. ÐŸÐ¾Ð¿Ñ€Ð¾Ð±ÑƒÐ¹ ÐµÑ‰Ñ‘ Ñ€Ð°Ð·.")
        return

    # ÐžÑ‚Ð²ÐµÑ‡Ð°ÐµÐ¼ Ð¿Ð¾ Ñ€Ð°ÑÐ¿Ð¾Ð·Ð½Ð°Ð½Ð½Ð¾Ð¼Ñƒ Ñ‚ÐµÐºÑÑ‚Ñƒ (Ð¸ Ð¼ÑÐ³ÐºÐ¾ Ð¿Ð¾ÐºÐ°Ð·Ñ‹Ð²Ð°ÐµÐ¼, Ñ‡Ñ‚Ð¾ Ð±Ñ‹Ð»Ð¾ Ð¿Ð¾Ð½ÑÑ‚Ð¾)
    prefix = f"ðŸ—£ï¸ Ð Ð°ÑÐ¿Ð¾Ð·Ð½Ð°Ð»: Â«{text}Â»\n\n"
    web_ctx = ""
    sources = []
    if should_browse(text):
        answer_from_search, results = tavily_search(text, max_results=5)
        sources = results or []
        ctx_lines = []
        if answer_from_search:
            ctx_lines.append(f"ÐšÑ€Ð°Ñ‚ÐºÐ°Ñ ÑÐ²Ð¾Ð´ÐºÐ° Ð¿Ð¾Ð¸ÑÐºÐ¾Ð¼: {answer_from_search}")
        for i, it in enumerate(sources, 1):
            ctx_lines.append(f"[{i}] {it.get('title','')}: {it.get('url','')}")
        web_ctx = "\n".join(ctx_lines)

    answer = await ask_openai_text(text, web_ctx=web_ctx)
    answer = prefix + answer + format_sources(sources)
    await update.message.reply_text(answer, disable_web_page_preview=False)

async def on_audio(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """ÐžÐ±Ñ‹Ñ‡Ð½Ñ‹Ðµ Ð°ÑƒÐ´Ð¸Ð¾-Ñ„Ð°Ð¹Ð»Ñ‹ (mp3/m4a/wav) â€” Ð¾Ð±Ñ€Ð°Ð±Ð°Ñ‚Ñ‹Ð²Ð°ÐµÐ¼ ÐºÐ°Ðº voice."""
    chat_id = update.effective_chat.id
    await typing(context, chat_id)

    audio = update.message.audio
    file = await context.bot.get_file(audio.file_id)
    buf = BytesIO()
    await file.download_to_memory(buf)

    # ÐŸÐ¾Ð¿Ñ€Ð¾Ð±ÑƒÐµÐ¼ ÑƒÐ³Ð°Ð´Ð°Ñ‚ÑŒ Ð¸Ð¼Ñ Ñ„Ð°Ð¹Ð»Ð° Ð¸Ð· Ð¿Ð¾Ð´Ð¿Ð¸ÑÐ¸/Ð¼ÐµÑ‚Ð°Ð´Ð°Ð½Ð½Ñ‹Ñ…
    filename = (audio.file_name or "audio.mp3")
    text = await transcribe_audio(buf, filename_hint=filename)
    if not text:
        await update.message.reply_text("ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ñ€Ð°ÑÐ¿Ð¾Ð·Ð½Ð°Ñ‚ÑŒ Ð°ÑƒÐ´Ð¸Ð¾. ÐŸÐ¾Ð¿Ñ€Ð¾Ð±ÑƒÐ¹ ÐµÑ‰Ñ‘ Ñ€Ð°Ð·.")
        return

    prefix = f"ðŸ—£ï¸ Ð Ð°ÑÐ¿Ð¾Ð·Ð½Ð°Ð»: Â«{text}Â»\n\n"
    web_ctx = ""
    sources = []
    if should_browse(text):
        answer_from_search, results = tavily_search(text, max_results=5)
        sources = results or []
        ctx_lines = []
        if answer_from_search:
            ctx_lines.append(f"ÐšÑ€Ð°Ñ‚ÐºÐ°Ñ ÑÐ²Ð¾Ð´ÐºÐ° Ð¿Ð¾Ð¸ÑÐºÐ¾Ð¼: {answer_from_search}")
        for i, it in enumerate(sources, 1):
            ctx_lines.append(f"[{i}] {it.get('title','')}: {it.get('url','')}")
        web_ctx = "\n".join(ctx_lines)

    answer = await ask_openai_text(text, web_ctx=web_ctx)
    answer = prefix + answer + format_sources(sources)
    await update.message.reply_text(answer, disable_web_page_preview=False)

# ========== BOOTSTRAP ==========
def build_app():
    app = ApplicationBuilder().token(BOT_TOKEN).build()
    app.add_handler(CommandHandler("start", cmd_start))
    # Ñ‚ÐµÐºÑÑ‚
    app.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, on_text))
    # Ñ„Ð¾Ñ‚Ð¾
    app.add_handler(MessageHandler(filters.PHOTO, on_photo))
    # Ð³Ð¾Ð»Ð¾ÑÐ¾Ð²Ñ‹Ðµ (voice) Ð¸ Ð°ÑƒÐ´Ð¸Ð¾
    app.add_handler(MessageHandler(filters.VOICE, on_voice))
    app.add_handler(MessageHandler(filters.AUDIO, on_audio))
    return app

def run_webhook(app):
    # ÑƒÐ½Ð¸ÐºÐ°Ð»ÑŒÐ½Ñ‹Ð¹ Ð¿ÑƒÑ‚ÑŒ (Ñ‡Ñ‚Ð¾Ð± Ð½Ð¸ÐºÑ‚Ð¾ ÑÐ»ÑƒÑ‡Ð°Ð¹Ð½Ð¾ Ð½Ðµ Ð´ÐµÑ€Ð³Ð°Ð»)
    url_path = f"webhook/{BOT_TOKEN}"
    webhook_url = f"{PUBLIC_URL.rstrip('/')}/{url_path}"

    log.info("Starting webhook on 0.0.0.0:%s  ->  %s", PORT, webhook_url)
    app.run_webhook(
        listen="0.0.0.0",
        port=PORT,
        url_path=url_path,
        webhook_url=webhook_url,
        secret_token=WEBHOOK_SECRET or None,   # Telegram header X-Telegram-Bot-Api-Secret-Token
        drop_pending_updates=True,
    )

def main():
    app = build_app()
    run_webhook(app)

if __name__ == "__main__":
    main()

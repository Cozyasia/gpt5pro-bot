# -*- coding: utf-8 -*-
import os
import re
import json
import base64
import logging
from io import BytesIO

import httpx
from telegram import Update, ReplyKeyboardMarkup, KeyboardButton, WebAppInfo
from telegram.ext import (
    ApplicationBuilder, CommandHandler, MessageHandler, ContextTypes, filters
)
from telegram.constants import ChatAction

# ========== LOGGING ==========
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s | %(levelname)s | %(name)s | %(message)s",
)
log = logging.getLogger("gpt-bot")

# ========== ENV ==========
BOT_TOKEN        = os.environ.get("BOT_TOKEN", "").strip()
PUBLIC_URL       = os.environ.get("PUBLIC_URL", "").strip()
WEBAPP_URL       = os.environ.get("WEBAPP_URL", "").strip()
OPENAI_API_KEY   = os.environ.get("OPENAI_API_KEY", "").strip()      # ÐºÐ»ÑŽÑ‡ Ð´Ð»Ñ LLM (OpenRouter Ð¸Ð»Ð¸ OpenAI)
OPENAI_BASE_URL  = os.environ.get("OPENAI_BASE_URL", "").strip()     # Ð´Ð»Ñ OpenRouter: https://openrouter.ai/api/v1
OPENAI_MODEL     = os.environ.get("OPENAI_MODEL", "openai/gpt-4o-mini").strip()

# Ð”Ð»Ñ Ð²ÐµÐ¶Ð»Ð¸Ð²Ñ‹Ñ… Ð·Ð°Ð³Ð¾Ð»Ð¾Ð²ÐºÐ¾Ð² OpenRouter (ÐºÐ°ÑÑ‚Ð¾Ð¼Ð½Ñ‹Ðµ, Ð½Ðµ Ð¾Ð±ÑÐ·Ð°Ñ‚ÐµÐ»ÑŒÐ½Ñ‹):
OPENROUTER_SITE_URL = os.environ.get("OPENROUTER_SITE_URL", "").strip()
OPENROUTER_APP_NAME = os.environ.get("OPENROUTER_APP_NAME", "").strip()

WEBHOOK_SECRET   = os.environ.get("WEBHOOK_SECRET", "").strip()
BANNER_URL       = os.environ.get("BANNER_URL", "").strip()
TAVILY_API_KEY   = os.environ.get("TAVILY_API_KEY", "").strip()

# STT:
DEEPGRAM_API_KEY = os.environ.get("DEEPGRAM_API_KEY", "").strip()
OPENAI_STT_KEY   = os.environ.get("OPENAI_STT_KEY", "").strip()      # Ð¾Ð±Ñ‹Ñ‡Ð½Ñ‹Ð¹ OpenAI ÐºÐ»ÑŽÑ‡ Ð´Ð»Ñ Whisper (Ð¾Ð¿Ñ†.)
TRANSCRIBE_MODEL = os.environ.get("OPENAI_TRANSCRIBE_MODEL", "whisper-1").strip()

PORT             = int(os.environ.get("PORT", "10000"))

if not BOT_TOKEN:
    raise RuntimeError("ENV BOT_TOKEN is required")
if not PUBLIC_URL or not PUBLIC_URL.startswith("http"):
    raise RuntimeError("ENV PUBLIC_URL must look like https://xxx.onrender.com")
if not OPENAI_API_KEY:
    raise RuntimeError("ENV OPENAI_API_KEY is required")

WEB_ROOT = WEBAPP_URL or PUBLIC_URL

# ========== OPENAI / Tavily clients ==========
from openai import OpenAI

# ÐšÐ»Ð¸ÐµÐ½Ñ‚ Ð´Ð»Ñ LLM (OpenRouter Ð¸Ð»Ð¸ OpenAI)
default_headers = {}
if OPENROUTER_SITE_URL:
    default_headers["HTTP-Referer"] = OPENROUTER_SITE_URL
if OPENROUTER_APP_NAME:
    default_headers["X-Title"] = OPENROUTER_APP_NAME

oai_llm = OpenAI(
    api_key=OPENAI_API_KEY,
    base_url=OPENAI_BASE_URL or None,               # ÐµÑÐ»Ð¸ Ð¿ÑƒÑÑ‚Ð¾ â€” api.openai.com
    default_headers=default_headers or None,
)

# ÐžÑ‚Ð´ÐµÐ»ÑŒÐ½Ñ‹Ð¹ ÐºÐ»Ð¸ÐµÐ½Ñ‚ Ð´Ð»Ñ Whisper (ÐµÑÐ»Ð¸ ÑƒÐºÐ°Ð·Ð°Ð½ OPENAI_STT_KEY, Ð²ÑÐµÐ³Ð´Ð° Ð¸Ð´Ñ‘Ð¼ Ð½Ð° api.openai.com)
oai_stt = None
if OPENAI_STT_KEY:
    oai_stt = OpenAI(api_key=OPENAI_STT_KEY)        # Ð²ÑÐµÐ³Ð´Ð° Ð¾Ñ„Ð¸Ñ†Ð¸Ð°Ð»ÑŒÐ½Ñ‹Ð¹ OpenAI Ð´Ð»Ñ Ð°ÑƒÐ´Ð¸Ð¾

# Tavily (Ð¿Ð¾Ð¸ÑÐº)
try:
    if TAVILY_API_KEY:
        from tavily import TavilyClient
        tavily = TavilyClient(api_key=TAVILY_API_KEY)
    else:
        tavily = None
except Exception:
    tavily = None

# ========== PROMPTS ==========
SYSTEM_PROMPT = (
    "Ð¢Ñ‹ Ð´Ñ€ÑƒÐ¶ÐµÐ»ÑŽÐ±Ð½Ñ‹Ð¹ Ð¸ Ð»Ð°ÐºÐ¾Ð½Ð¸Ñ‡Ð½Ñ‹Ð¹ Ð°ÑÑÐ¸ÑÑ‚ÐµÐ½Ñ‚ Ð½Ð° Ñ€ÑƒÑÑÐºÐ¾Ð¼. "
    "ÐžÑ‚Ð²ÐµÑ‡Ð°Ð¹ Ð¿Ð¾ ÑÑƒÑ‚Ð¸, ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ð¸Ñ€ÑƒÐ¹ ÑÐ¿Ð¸ÑÐºÐ°Ð¼Ð¸/ÑˆÐ°Ð³Ð°Ð¼Ð¸, Ð½Ðµ Ð²Ñ‹Ð´ÑƒÐ¼Ñ‹Ð²Ð°Ð¹ Ñ„Ð°ÐºÑ‚Ñ‹. "
    "Ð•ÑÐ»Ð¸ ÑÑÑ‹Ð»Ð°ÐµÑˆÑŒÑÑ Ð½Ð° Ð¸ÑÑ‚Ð¾Ñ‡Ð½Ð¸ÐºÐ¸ â€” Ð² ÐºÐ¾Ð½Ñ†Ðµ Ð´Ð°Ð¹ ÐºÐ¾Ñ€Ð¾Ñ‚ÐºÐ¸Ð¹ ÑÐ¿Ð¸ÑÐ¾Ðº ÑÑÑ‹Ð»Ð¾Ðº."
)

VISION_SYSTEM_PROMPT = (
    "Ð¢Ñ‹ Ñ‡Ñ‘Ñ‚ÐºÐ¾ Ð¾Ð¿Ð¸ÑÑ‹Ð²Ð°ÐµÑˆÑŒ ÑÐ¾Ð´ÐµÑ€Ð¶Ð¸Ð¼Ð¾Ðµ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ð¹: Ð¾Ð±ÑŠÐµÐºÑ‚Ñ‹, Ñ‚ÐµÐºÑÑ‚, ÑÑ…ÐµÐ¼Ñ‹, Ð³Ñ€Ð°Ñ„Ð¸ÐºÐ¸. "
    "ÐÐµ Ð¸Ð´ÐµÐ½Ñ‚Ð¸Ñ„Ð¸Ñ†Ð¸Ñ€ÑƒÐ¹ Ð»Ð¸Ñ‡Ð½Ð¾ÑÑ‚Ð¸ Ð»ÑŽÐ´ÐµÐ¹ Ð¸ Ð½Ðµ Ð¿Ð¸ÑˆÐ¸ Ð¸Ð¼ÐµÐ½Ð°, ÐµÑÐ»Ð¸ Ð¾Ð½Ð¸ Ð½Ðµ Ð½Ð°Ð¿ÐµÑ‡Ð°Ñ‚Ð°Ð½Ñ‹ Ð½Ð° Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ð¸."
)

VISION_CAPABILITY_HELP = (
    "Ð”Ð° â€” Ð°Ð½Ð°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÑŽ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ñ Ð¸ Ð¿Ð¾Ð¼Ð¾Ð³Ð°ÑŽ Ñ Ð²Ð¸Ð´ÐµÐ¾ Ð¿Ð¾ ÐºÐ°Ð´Ñ€Ð°Ð¼, Ð° ÐµÑ‰Ñ‘ Ñ€Ð°ÑÐ¿Ð¾Ð·Ð½Ð°ÑŽ Ð³Ð¾Ð»Ð¾Ñ. âœ…\n\n"
    "â€¢ Ð¤Ð¾Ñ‚Ð¾/ÑÐºÑ€Ð¸Ð½ÑˆÐ¾Ñ‚Ñ‹: JPG/PNG/WebP (Ð´Ð¾ ~10 ÐœÐ‘) â€” Ð¾Ð¿Ð¸ÑˆÑƒ, Ð¿Ñ€Ð¾Ñ‡Ð¸Ñ‚Ð°ÑŽ Ñ‚ÐµÐºÑÑ‚, Ñ€Ð°Ð·Ð±ÐµÑ€Ñƒ Ð³Ñ€Ð°Ñ„Ð¸ÐºÐ¸.\n"
    "â€¢ Ð”Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ñ‹/PDF: Ð¿Ñ€Ð¸ÑˆÐ»Ð¸ ÐºÐ°Ðº *Ñ„Ð°Ð¹Ð»*, Ð¸Ð·Ð²Ð»ÐµÐºÑƒ Ñ‚ÐµÐºÑÑ‚/Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ñ‹.\n"
    "â€¢ Ð’Ð¸Ð´ÐµÐ¾: Ð¿Ñ€Ð¸ÑˆÐ»Ð¸ 1â€“3 ÐºÐ»ÑŽÑ‡ÐµÐ²Ñ‹Ñ… ÐºÐ°Ð´Ñ€Ð° (ÑÐºÑ€Ð¸Ð½ÑˆÐ¾Ñ‚Ð°) â€” Ð¿Ñ€Ð¾Ð°Ð½Ð°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÑŽ Ð¿Ð¾ ÐºÐ°Ð´Ñ€Ð°Ð¼.\n"
    "â€¢ Ð“Ð¾Ð»Ð¾ÑÐ¾Ð²Ñ‹Ðµ/Ð°ÑƒÐ´Ð¸Ð¾: Ñ€Ð°ÑÐ¿Ð¾Ð·Ð½Ð°ÑŽ Ñ€ÐµÑ‡ÑŒ Ð¸ Ð¾Ñ‚Ð²ÐµÑ‡Ñƒ Ð¿Ð¾ ÑÐ¾Ð´ÐµÑ€Ð¶Ð°Ð½Ð¸ÑŽ."
)

# ========== HEURISTICS ==========
_SMALLTALK_RE = re.compile(
    r"^(Ð¿Ñ€Ð¸Ð²ÐµÑ‚|Ð·Ð´Ñ€Ð°Ð²ÑÑ‚Ð²ÑƒÐ¹|Ð´Ð¾Ð±Ñ€Ñ‹Ð¹\s*(Ð´ÐµÐ½ÑŒ|Ð²ÐµÑ‡ÐµÑ€|ÑƒÑ‚Ñ€Ð¾)|Ñ…Ð¸|hi|hello|Ñ…ÐµÐ»Ð»Ð¾|ÐºÐ°Ðº Ð´ÐµÐ»Ð°|ÑÐ¿Ð°ÑÐ¸Ð±Ð¾|Ð¿Ð¾ÐºÐ°)\b",
    re.IGNORECASE
)
_NEWSY_RE = re.compile(
    r"(ÐºÐ¾Ð³Ð´Ð°|Ð´Ð°Ñ‚Ð°|Ð²Ñ‹Ð¹Ð´ÐµÑ‚|Ñ€ÐµÐ»Ð¸Ð·|Ð½Ð¾Ð²Ð¾ÑÑ‚|ÐºÑƒÑ€Ñ|Ñ†ÐµÐ½Ð°|Ð¿Ñ€Ð¾Ð³Ð½Ð¾Ð·|Ñ‡Ñ‚Ð¾ Ñ‚Ð°ÐºÐ¾Ðµ|ÐºÑ‚Ð¾ Ñ‚Ð°ÐºÐ¾Ð¹|Ð½Ð°Ð¹Ð´Ð¸|ÑÑÑ‹Ð»ÐºÐ°|Ð¾Ñ„Ð¸Ñ†Ð¸Ð°Ð»|Ð°Ð´Ñ€ÐµÑ|Ñ‚ÐµÐ»ÐµÑ„Ð¾Ð½|"
    r"Ð¿Ð¾Ð³Ð¾Ð´Ð°|ÑÐµÐ³Ð¾Ð´Ð½Ñ|ÑÐµÐ¹Ñ‡Ð°Ñ|ÑˆÑ‚Ñ€Ð°Ñ„|Ð·Ð°ÐºÐ¾Ð½|Ñ‚Ñ€ÐµÐ½Ð´|ÐºÐ¾Ñ‚Ð¸Ñ€Ð¾Ð²Ðº|Ð¾Ð±Ð·Ð¾Ñ€|Ñ€Ð°ÑÐ¿Ð¸ÑÐ°Ð½Ð¸|Ð·Ð°Ð¿ÑƒÑÐº|update|Ð½Ð¾Ð²Ð°Ñ Ð²ÐµÑ€ÑÐ¸Ñ)",
    re.IGNORECASE
)
_CAPABILITY_RE = re.compile(
    r"(Ð¼Ð¾Ð¶(ÐµÑˆÑŒ|Ð½Ð¾).{0,10}(Ð°Ð½Ð°Ð»Ð¸Ð·(Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ)?|Ñ€Ð°ÑÐ¿Ð¾Ð·Ð½Ð°Ð²(Ð°Ñ‚ÑŒ|Ð°Ð½Ð¸Ðµ)).{0,10}(Ñ„Ð¾Ñ‚Ð¾|ÐºÐ°Ñ€Ñ‚Ð¸Ð½Ðº|Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½|image|picture)|"
    r"Ð°Ð½Ð°Ð»Ð¸Ð·(Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ)?.{0,8}(Ñ„Ð¾Ñ‚Ð¾|ÐºÐ°Ñ€Ñ‚Ð¸Ð½Ðº|Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½)|"
    r"(Ð¼Ð¾Ð¶(ÐµÑˆÑŒ|Ð½Ð¾).{0,10})?(Ð°Ð½Ð°Ð»Ð¸Ð·|Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ñ‚ÑŒ).{0,6}Ñ.{0,6}Ð²Ð¸Ð´ÐµÐ¾)",
    re.IGNORECASE
)

def is_smalltalk(text: str) -> bool:
    return bool(_SMALLTALK_RE.search(text.strip()))

def should_browse(text: str) -> bool:
    t = text.strip()
    if is_smalltalk(t):
        return False
    return bool(_NEWSY_RE.search(t) or "?" in t or len(t) > 80)

def is_vision_capability_question(text: str) -> bool:
    return bool(_CAPABILITY_RE.search(text))

# ========== UTILS ==========
async def typing(ctx: ContextTypes.DEFAULT_TYPE, chat_id: int):
    try:
        await ctx.bot.send_chat_action(chat_id, action=ChatAction.TYPING)
    except Exception:
        pass

def sniff_image_mime(data: bytes) -> str:
    if data.startswith(b"\xff\xd8"):
        return "image/jpeg"
    if data.startswith(b"\x89PNG"):
        return "image/png"
    if data[:4] == b"RIFF" and b"WEBP" in data[:16]:
        return "image/webp"
    return "image/jpeg"

def format_sources(items):
    if not items:
        return ""
    lines = []
    for i, it in enumerate(items, 1):
        title = it.get("title") or it.get("url") or "Ð˜ÑÑ‚Ð¾Ñ‡Ð½Ð¸Ðº"
        url = it.get("url") or ""
        lines.append(f"[{i}] {title} â€” {url}")
    return "\n\nÐ¡ÑÑ‹Ð»ÐºÐ¸:\n" + "\n".join(lines)

def tavily_search(query: str, max_results: int = 5):
    if not tavily:
        return None, []
    try:
        res = tavily.search(
            query=query,
            search_depth="advanced",
            max_results=max_results,
            include_answer=True,
            include_raw_content=False,
        )
        answer = res.get("answer") or ""
        results = res.get("results") or []
        return answer, results
    except Exception as e:
        log.exception("Tavily error: %s", e)
        return None, []

# ========== OpenAI helpers ==========
async def ask_openai_text(user_text: str, web_ctx: str = "") -> str:
    messages = [{"role": "system", "content": SYSTEM_PROMPT}]
    if web_ctx:
        messages.append({"role": "system", "content": f"ÐšÐ¾Ð½Ñ‚ÐµÐºÑÑ‚ Ð¸Ð· Ð²ÐµÐ±-Ð¿Ð¾Ð¸ÑÐºÐ°:\n{web_ctx}"})
    messages.append({"role": "user", "content": user_text})

    try:
        resp = oai_llm.chat.completions.create(
            model=OPENAI_MODEL,
            messages=messages,
            temperature=0.6,
        )
        return (resp.choices[0].message.content or "").strip()
    except Exception as e:
        log.exception("OpenAI chat error: %s", e)
        return "ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ð¿Ð¾Ð»ÑƒÑ‡Ð¸Ñ‚ÑŒ Ð¾Ñ‚Ð²ÐµÑ‚ Ð¾Ñ‚ Ð¼Ð¾Ð´ÐµÐ»Ð¸ (Ð»Ð¸Ð¼Ð¸Ñ‚/ÐºÐ»ÑŽÑ‡). ÐŸÐ¾Ð¿Ñ€Ð¾Ð±ÑƒÐ¹ Ð¿Ð¾Ð·Ð¶Ðµ."

async def ask_openai_vision(user_text: str, img_b64: str, mime: str) -> str:
    try:
        resp = oai_llm.chat.completions.create(
            model=OPENAI_MODEL,
            messages=[
                {"role": "system", "content": VISION_SYSTEM_PROMPT},
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": user_text or "ÐžÐ¿Ð¸ÑˆÐ¸, Ñ‡Ñ‚Ð¾ Ð½Ð° Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ð¸ Ð¸ ÐºÐ°ÐºÐ¾Ð¹ Ñ‚Ð°Ð¼ Ñ‚ÐµÐºÑÑ‚."},
                        {"type": "image_url",
                         "image_url": {"url": f"data:{mime};base64,{img_b64}"}}
                    ]
                }
            ],
            temperature=0.4,
        )
        return (resp.choices[0].message.content or "").strip()
    except Exception as e:
        log.exception("Vision error: %s", e)
        return "ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ð¿Ñ€Ð¾Ð°Ð½Ð°Ð»Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ðµ (Ð»Ð¸Ð¼Ð¸Ñ‚/ÐºÐ»ÑŽÑ‡). ÐŸÐ¾Ð¿Ñ€Ð¾Ð±ÑƒÐ¹ Ð¿Ð¾Ð·Ð¶Ðµ."

# ========== STT: Deepgram -> Whisper(OpenAI) fallback ==========
async def transcribe_audio(buf: BytesIO, filename_hint: str = "audio.ogg") -> str:
    data = buf.getvalue()

    # 1) Deepgram
    if DEEPGRAM_API_KEY:
        try:
            async with httpx.AsyncClient(timeout=60.0) as client:
                params = {
                    "model": "nova-2",
                    "language": "ru",
                    "smart_format": "true",
                    "punctuate": "true",
                }
                headers = {
                    "Authorization": f"Token {DEEPGRAM_API_KEY}",
                    "Content-Type": "audio/ogg" if filename_hint.endswith(".ogg") else "application/octet-stream",
                }
                r = await client.post(
                    "https://api.deepgram.com/v1/listen",
                    params=params,
                    headers=headers,
                    content=data
                )
                r.raise_for_status()
                dg = r.json()
                text = (
                    dg.get("results", {})
                      .get("channels", [{}])[0]
                      .get("alternatives", [{}])[0]
                      .get("transcript", "")
                ).strip()
                if text:
                    return text
        except Exception as e:
            log.exception("Deepgram STT error: %s", e)

    # 2) Whisper (Ð¢ÐžÐ›Ð¬ÐšÐž ÐµÑÐ»Ð¸ ÐµÑÑ‚ÑŒ Ð¾Ñ‚Ð´ÐµÐ»ÑŒÐ½Ñ‹Ð¹ ÐºÐ»ÑŽÑ‡ OpenAI)
    if oai_stt:
        try:
            buf2 = BytesIO(data); buf2.seek(0)
            setattr(buf2, "name", filename_hint)
            tr = oai_stt.audio.transcriptions.create(
                model=TRANSCRIBE_MODEL,  # whisper-1
                file=buf2
            )
            return (tr.text or "").strip()
        except Exception as e:
            log.exception("Whisper STT error: %s", e)

    return ""

# ========== STATIC TEXTS ==========
START_TEXT = "ÐŸÑ€Ð¸Ð²ÐµÑ‚! Ð¯ Ð³Ð¾Ñ‚Ð¾Ð². Ð§ÐµÐ¼ Ð¿Ð¾Ð¼Ð¾Ñ‡ÑŒ?"

MODES_TEXT = (
    "âš™ï¸ *Ð ÐµÐ¶Ð¸Ð¼Ñ‹ Ñ€Ð°Ð±Ð¾Ñ‚Ñ‹*\n"
    "â€¢ ðŸ’¬ Ð£Ð½Ð¸Ð²ÐµÑ€ÑÐ°Ð»ÑŒÐ½Ñ‹Ð¹ â€” Ð¾Ð±Ñ‹Ñ‡Ð½Ñ‹Ð¹ Ð´Ð¸Ð°Ð»Ð¾Ð³.\n"
    "â€¢ ðŸ§  Ð˜ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒ â€” Ñ„Ð°ÐºÑ‚Ñ‹/Ð¸ÑÑ‚Ð¾Ñ‡Ð½Ð¸ÐºÐ¸, ÑÐ²Ð¾Ð´ÐºÐ¸.\n"
    "â€¢ âœï¸ Ð ÐµÐ´Ð°ÐºÑ‚Ð¾Ñ€ â€” Ð¿Ñ€Ð°Ð²ÐºÐ¸ Ñ‚ÐµÐºÑÑ‚Ð°, ÑÑ‚Ð¸Ð»ÑŒ, ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ð°.\n"
    "â€¢ ðŸ“Š ÐÐ½Ð°Ð»Ð¸Ñ‚Ð¸Ðº â€” Ñ„Ð¾Ñ€Ð¼ÑƒÐ»Ñ‹, Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ñ‹, Ñ€Ð°ÑÑ‡Ñ‘Ñ‚Ð½Ñ‹Ðµ ÑˆÐ°Ð³Ð¸.\n"
    "â€¢ ðŸ–¼ï¸ Ð’Ð¸Ð·ÑƒÐ°Ð»ÑŒÐ½Ñ‹Ð¹ â€” Ð¾Ð¿Ð¸ÑÐ°Ð½Ð¸Ðµ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ð¹, OCR, ÑÑ…ÐµÐ¼Ñ‹.\n"
    "â€¢ ðŸŽ™ï¸ Ð“Ð¾Ð»Ð¾Ñ â€” Ñ€Ð°ÑÐ¿Ð¾Ð·Ð½Ð°ÑŽ Ð°ÑƒÐ´Ð¸Ð¾ Ð¸ Ð¾Ñ‚Ð²ÐµÑ‡Ð°ÑŽ Ð¿Ð¾ ÑÐ¾Ð´ÐµÑ€Ð¶Ð°Ð½Ð¸ÑŽ.\n\n"
    "_Ð’Ñ‹Ð±Ð¸Ñ€Ð°Ð¹ Ñ€ÐµÐ¶Ð¸Ð¼ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸ÐµÐ¼ Ð¸Ð»Ð¸ Ð¿Ñ€Ð¾ÑÑ‚Ð¾ ÑÑ„Ð¾Ñ€Ð¼ÑƒÐ»Ð¸Ñ€ÑƒÐ¹ Ð·Ð°Ð´Ð°Ñ‡Ñƒ._"
)

EXAMPLES_TEXT = (
    "ðŸ§© *ÐŸÑ€Ð¸Ð¼ÐµÑ€Ñ‹ Ð·Ð°Ð¿Ñ€Ð¾ÑÐ¾Ð²*\n"
    "â€¢ Â«Ð¡Ð´ÐµÐ»Ð°Ð¹ ÐºÐ¾Ð½ÑÐ¿ÐµÐºÑ‚ Ð³Ð»Ð°Ð²Ñ‹ 3 Ð¸ Ð²Ñ‹Ð´ÐµÐ»Ð¸ Ñ„Ð¾Ñ€Ð¼ÑƒÐ»Ñ‹Â»\n"
    "â€¢ Â«ÐŸÑ€Ð¾Ð°Ð½Ð°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÐ¹ CSV, Ð½Ð°Ð¹Ð´Ð¸ Ñ‚Ñ€ÐµÐ½Ð´Ñ‹ Ð¸ ÑÐ´ÐµÐ»Ð°Ð¹ ÐºÑ€Ð°Ñ‚ÐºÐ¸Ð¹ Ð²Ñ‹Ð²Ð¾Ð´Â»\n"
    "â€¢ Â«Ð¡Ð¾ÑÑ‚Ð°Ð²ÑŒ Ð¿Ð¸ÑÑŒÐ¼Ð¾ ÐºÐ»Ð¸ÐµÐ½Ñ‚Ñƒ, Ð´Ñ€ÑƒÐ¶ÐµÐ»ÑŽÐ±Ð½Ð¾ Ð¸ Ð¿Ð¾ Ð´ÐµÐ»ÑƒÂ»\n"
    "â€¢ Â«Ð¡ÑƒÐ¼Ð¼Ð¸Ñ€ÑƒÐ¹ ÑÑ‚Ð°Ñ‚ÑŒÑŽ Ð¸Ð· ÑÑÑ‹Ð»ÐºÐ¸ Ð¸ Ð´Ð°Ð¹ Ð¸ÑÑ‚Ð¾Ñ‡Ð½Ð¸ÐºÐ¸Â»\n"
    "â€¢ Â«ÐžÐ¿Ð¸ÑˆÐ¸ Ñ‚ÐµÐºÑÑ‚ Ð½Ð° Ñ„Ð¾Ñ‚Ð¾ Ð¸ Ð¸Ð·Ð²Ð»ÐµÐºÐ¸ Ñ‚Ð°Ð±Ð»Ð¸Ñ†ÑƒÂ»"
)

# ========== START UI / KEYBOARD ==========
main_kb = ReplyKeyboardMarkup(
    [
        [KeyboardButton("ðŸ§­ ÐœÐµÐ½ÑŽ", web_app=WebAppInfo(url=WEB_ROOT))],
        [KeyboardButton("âš™ï¸ Ð ÐµÐ¶Ð¸Ð¼Ñ‹"), KeyboardButton("ðŸ§© ÐŸÑ€Ð¸Ð¼ÐµÑ€Ñ‹")],
        [KeyboardButton("â­ ÐŸÐ¾Ð´Ð¿Ð¸ÑÐºÐ°", web_app=WebAppInfo(url=f"{WEB_ROOT}/premium.html"))],
    ],
    resize_keyboard=True
)

# ========== HANDLERS ==========
async def cmd_start(update: Update, context: ContextTypes.DEFAULT_TYPE):
    if BANNER_URL:
        try:
            await update.effective_message.reply_photo(BANNER_URL)
        except Exception:
            pass
    await update.effective_message.reply_text(START_TEXT, reply_markup=main_kb, disable_web_page_preview=True)

async def cmd_modes(update: Update, context: ContextTypes.DEFAULT_TYPE):
    await update.effective_message.reply_text(MODES_TEXT, disable_web_page_preview=True, parse_mode="Markdown")

async def cmd_examples(update: Update, context: ContextTypes.DEFAULT_TYPE):
    await update.effective_message.reply_text(EXAMPLES_TEXT, disable_web_page_preview=True, parse_mode="Markdown")

async def handle_web_app_data(update: Update, context: ContextTypes.DEFAULT_TYPE):
    msg = update.effective_message
    wad = getattr(msg, "web_app_data", None)
    if not wad:
        return
    raw = wad.data or ""
    try:
        payload = json.loads(raw) if raw.strip().startswith("{") else {"type": raw}
    except Exception:
        payload = {"type": str(raw)}

    ptype = (payload.get("type") or "").strip().lower()
    log.info("web_app_data: %s", payload)

    if ptype in ("help_from_webapp", "help", "question"):
        await msg.reply_text(
            "ðŸ§‘â€ðŸ’» ÐŸÐ¾Ð´Ð´ÐµÑ€Ð¶ÐºÐ° GPT-5 PRO.\nÐÐ°Ð¿Ð¸ÑˆÐ¸ Ð·Ð´ÐµÑÑŒ ÑÐ²Ð¾Ð¹ Ð²Ð¾Ð¿Ñ€Ð¾Ñ â€” Ð¾Ñ‚Ð²ÐµÑ‡Ñƒ Ð² Ñ‡Ð°Ñ‚Ðµ.\n\n"
            "Ð¢Ð°ÐºÐ¶Ðµ Ð¼Ð¾Ð¶Ð½Ð¾ Ð½Ð° Ð¿Ð¾Ñ‡Ñ‚Ñƒ: sale.rielt@bk.ru"
        )
        return

    if ptype in ("plan_from_webapp", "plan", "subscribe", "subscription"):
        kb = ReplyKeyboardMarkup(
            [[KeyboardButton("â­ ÐžÑ‚ÐºÑ€Ñ‹Ñ‚ÑŒ Ð¿Ð¾Ð´Ð¿Ð¸ÑÐºÑƒ", web_app=WebAppInfo(url=f"{WEB_ROOT}/premium.html"))]],
            resize_keyboard=True, one_time_keyboard=True
        )
        await msg.reply_text("ÐžÑ„Ð¾Ñ€Ð¼Ð¸Ñ‚ÑŒ Ð¿Ð¾Ð´Ð¿Ð¸ÑÐºÑƒ Ð¼Ð¾Ð¶Ð½Ð¾ Ð¿Ð¾ ÐºÐ½Ð¾Ð¿ÐºÐµ Ð½Ð¸Ð¶Ðµ. â¤µï¸", reply_markup=kb)
        return

    await msg.reply_text("ÐžÑ‚ÐºÑ€Ñ‹Ð» Ð±Ð¾Ñ‚Ð°. Ð§ÐµÐ¼ Ð¿Ð¾Ð¼Ð¾Ñ‡ÑŒ?", reply_markup=main_kb)

async def on_text(update: Update, context: ContextTypes.DEFAULT_TYPE):
    text = (update.message.text or "").strip()
    chat_id = update.effective_chat.id

    lower = text.lower()
    if lower in ("âš™ï¸ Ñ€ÐµÐ¶Ð¸Ð¼Ñ‹", "Ñ€ÐµÐ¶Ð¸Ð¼Ñ‹", "/modes"):
        await cmd_modes(update, context); return
    if lower in ("ðŸ§© Ð¿Ñ€Ð¸Ð¼ÐµÑ€Ñ‹", "Ð¿Ñ€Ð¸Ð¼ÐµÑ€Ñ‹", "/examples"):
        await cmd_examples(update, context); return

    if is_vision_capability_question(text):
        await update.message.reply_text(VISION_CAPABILITY_HELP, disable_web_page_preview=True); return

    await typing(context, chat_id)

    if is_smalltalk(text):
        reply = await ask_openai_text(text)
        await update.message.reply_text(reply); return

    web_ctx = ""
    sources = []
    if should_browse(text):
        ans, results = tavily_search(text, max_results=5)
        sources = results or []
        ctx_lines = []
        if ans: ctx_lines.append(f"ÐšÑ€Ð°Ñ‚ÐºÐ°Ñ ÑÐ²Ð¾Ð´ÐºÐ° Ð¿Ð¾Ð¸ÑÐºÐ¾Ð¼: {ans}")
        for i, it in enumerate(sources, 1):
            ctx_lines.append(f"[{i}] {it.get('title','')}: {it.get('url','')}")
        web_ctx = "\n".join(ctx_lines)

    answer = await ask_openai_text(text, web_ctx=web_ctx)
    if sources:
        answer += "\n\n" + "\n".join([f"[{i+1}] {s.get('title','')} â€” {s.get('url','')}" for i, s in enumerate(sources)])
    await update.message.reply_text(answer, disable_web_page_preview=False)

async def _handle_image_bytes(update: Update, context: ContextTypes.DEFAULT_TYPE, data: bytes, user_text: str):
    mime = sniff_image_mime(data)
    img_b64 = base64.b64encode(data).decode("ascii")
    answer = await ask_openai_vision(user_text, img_b64, mime)
    await update.message.reply_text(answer, disable_web_page_preview=True)

async def on_photo(update: Update, context: ContextTypes.DEFAULT_TYPE):
    chat_id = update.effective_chat.id
    await typing(context, chat_id)

    photo = update.message.photo[-1]
    file = await context.bot.get_file(photo.file_id)
    buf = BytesIO(); await file.download_to_memory(buf)
    user_text = (update.message.caption or "").strip()
    await _handle_image_bytes(update, context, buf.getvalue(), user_text)

async def on_document(update: Update, context: ContextTypes.DEFAULT_TYPE):
    chat_id = update.effective_chat.id
    await typing(context, chat_id)

    doc = update.message.document
    mime = (doc.mime_type or "").lower()
    if mime.startswith("image/"):
        file = await context.bot.get_file(doc.file_id)
        buf = BytesIO(); await file.download_to_memory(buf)
        user_text = (update.message.caption or "").strip()
        await _handle_image_bytes(update, context, buf.getvalue(), user_text)
    else:
        await update.message.reply_text(
            "Ð¤Ð°Ð¹Ð» Ð¿Ð¾Ð»ÑƒÑ‡Ð¸Ð». Ð•ÑÐ»Ð¸ ÑÑ‚Ð¾ PDF/Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚ â€” Ð¿Ñ€Ð¸ÑˆÐ»Ð¸ ÐºÐ¾Ð½ÐºÑ€ÐµÑ‚Ð½Ñ‹Ðµ ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ñ‹ ÐºÐ°Ðº Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ñ Ð¸Ð»Ð¸ ÑƒÐºÐ°Ð¶Ð¸, Ñ‡Ñ‚Ð¾ Ð¸Ð·Ð²Ð»ÐµÑ‡ÑŒ."
        )

async def on_voice(update: Update, context: ContextTypes.DEFAULT_TYPE):
    chat_id = update.effective_chat.id
    await typing(context, chat_id)

    voice = update.message.voice
    file = await context.bot.get_file(voice.file_id)
    buf = BytesIO(); await file.download_to_memory(buf)

    text = await transcribe_audio(buf, filename_hint="audio.ogg")
    if not text:
        await update.message.reply_text("ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ñ€Ð°ÑÐ¿Ð¾Ð·Ð½Ð°Ñ‚ÑŒ Ð³Ð¾Ð»Ð¾Ñ. ÐŸÐ¾Ð¿Ñ€Ð¾Ð±ÑƒÐ¹ ÐµÑ‰Ñ‘ Ñ€Ð°Ð·."); return

    prefix = f"ðŸ—£ï¸ Ð Ð°ÑÐ¿Ð¾Ð·Ð½Ð°Ð»: Â«{text}Â»\n\n"

    web_ctx = ""
    sources = []
    if should_browse(text):
        ans, results = tavily_search(text, max_results=5)
        sources = results or []
        ctx_lines = []
        if ans: ctx_lines.append(f"ÐšÑ€Ð°Ñ‚ÐºÐ°Ñ ÑÐ²Ð¾Ð´ÐºÐ° Ð¿Ð¾Ð¸ÑÐºÐ¾Ð¼: {ans}")
        for i, it in enumerate(sources, 1):
            ctx_lines.append(f"[{i}] {it.get('title','')}: {it.get('url','')}")
        web_ctx = "\n".join(ctx_lines)

    answer = await ask_openai_text(text, web_ctx=web_ctx)
    if sources:
        answer += "\n\n" + "\n".join([f"[{i+1}] {s.get('title','')} â€” {s.get('url','')}" for i, s in enumerate(sources)])
    await update.message.reply_text(prefix + answer, disable_web_page_preview=False)

async def on_audio(update: Update, context: ContextTypes.DEFAULT_TYPE):
    chat_id = update.effective_chat.id
    await typing(context, chat_id)

    audio = update.message.audio
    file = await context.bot.get_file(audio.file_id)
    buf = BytesIO(); await file.download_to_memory(buf)

    filename = (audio.file_name or "audio.mp3")
    text = await transcribe_audio(buf, filename_hint=filename)
    if not text:
        await update.message.reply_text("ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ñ€Ð°ÑÐ¿Ð¾Ð·Ð½Ð°Ñ‚ÑŒ Ð°ÑƒÐ´Ð¸Ð¾. ÐŸÐ¾Ð¿Ñ€Ð¾Ð±ÑƒÐ¹ ÐµÑ‰Ñ‘ Ñ€Ð°Ð·."); return

    prefix = f"ðŸ—£ï¸ Ð Ð°ÑÐ¿Ð¾Ð·Ð½Ð°Ð»: Â«{text}Â»\n\n"

    web_ctx = ""
    sources = []
    if should_browse(text):
        ans, results = tavily_search(text, max_results=5)
        sources = results or []
        ctx_lines = []
        if ans: ctx_lines.append(f"ÐšÑ€Ð°Ñ‚ÐºÐ°Ñ ÑÐ²Ð¾Ð´ÐºÐ° Ð¿Ð¾Ð¸ÑÐºÐ¾Ð¼: {ans}")
        for i, it in enumerate(sources, 1):
            ctx_lines.append(f"[{i}] {it.get('title','')}: {it.get('url','')}")
        web_ctx = "\n".join(ctx_lines)

    answer = await ask_openai_text(text, web_ctx=web_ctx)
    if sources:
        answer += "\n\n" + "\n".join([f"[{i+1}] {s.get('title','')} â€” {s.get('url','')}" for i, s in enumerate(sources)])
    await update.message.reply_text(prefix + answer, disable_web_page_preview=False)

async def on_video(update: Update, context: ContextTypes.DEFAULT_TYPE):
    await update.message.reply_text(
        "Ð”Ð°, Ð¿Ð¾Ð¼Ð¾Ð³Ñƒ Ñ Ð²Ð¸Ð´ÐµÐ¾: Ð¿Ñ€Ð¸ÑˆÐ»Ð¸ 1â€“3 ÐºÐ»ÑŽÑ‡ÐµÐ²Ñ‹Ñ… ÐºÐ°Ð´Ñ€Ð° (ÑÐºÑ€Ð¸Ð½ÑˆÐ¾Ñ‚Ð°) â€” Ð¿Ñ€Ð¾Ð°Ð½Ð°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÑŽ Ð¿Ð¾ ÐºÐ°Ð´Ñ€Ð°Ð¼ Ð¸ Ð¾Ñ‚Ð²ÐµÑ‡Ñƒ Ð¿Ð¾ ÑÐ¾Ð´ÐµÑ€Ð¶Ð°Ð½Ð¸ÑŽ. ðŸ“½ï¸"
    )

# ========== BOOTSTRAP ==========
def build_app():
    app = ApplicationBuilder().token(BOT_TOKEN).build()

    app.add_handler(CommandHandler("start", cmd_start))
    app.add_handler(CommandHandler("modes", cmd_modes))
    app.add_handler(CommandHandler("examples", cmd_examples))

    app.add_handler(MessageHandler(filters.StatusUpdate.WEB_APP_DATA, handle_web_app_data))
    app.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, on_text))
    app.add_handler(MessageHandler(filters.PHOTO, on_photo))
    app.add_handler(MessageHandler(filters.Document.IMAGE, on_document))
    app.add_handler(MessageHandler(filters.VOICE, on_voice))
    app.add_handler(MessageHandler(filters.AUDIO, on_audio))
    app.add_handler(MessageHandler(filters.VIDEO, on_video))
    return app

def run_webhook(app):
    url_path = f"webhook/{BOT_TOKEN}"
    webhook_url = f"{PUBLIC_URL.rstrip('/')}/{url_path}"

    log.info("Starting webhook on 0.0.0.0:%s  ->  %s", PORT, webhook_url)
    app.run_webhook(
        listen="0.0.0.0",
        port=PORT,
        url_path=url_path,
        webhook_url=webhook_url,
        secret_token=WEBHOOK_SECRET or None,
        drop_pending_updates=True,
    )

def main():
    app = build_app()
    run_webhook(app)

if __name__ == "__main__":
    main()

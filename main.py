# -*- coding: utf-8 -*-
"""
GPT5 PRO Telegram Bot
- python-telegram-bot==21.6
- openai>=1.51.0
Features:
â€¢ Positive image capabilities responses (text & voice)
â€¢ TTS with streaming and MP3 fallback
â€¢ STT for voice
â€¢ Photo quick actions: remove BG, replace BG, Outpaint, Animate (stub), Camera (stub), Storyboard
â€¢ /img generation, /plans with CryptoBot, /ver
"""
import os, sys, io, re, json, base64, sqlite3, asyncio, contextlib, uuid, logging
from datetime import datetime, timezone, timedelta
from typing import Optional, Dict, Any

# Heavy imports only executed in runtime, not in this build environment
from PIL import Image, ImageDraw  # Pillow
import httpx

from telegram import (
    Update, ReplyKeyboardMarkup, KeyboardButton, InlineKeyboardMarkup,
    InlineKeyboardButton, InputFile
)
from telegram.ext import (
    Application, CommandHandler, MessageHandler, CallbackQueryHandler,
    ContextTypes, filters
)

# ===== Logging =====
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s | %(levelname)s | %(name)s | %(message)s"
)
log = logging.getLogger("gpt5pro-bot")

# ===== Version =====
VERSION_TAG = "gpt5pro-main 2025-11-08-16:45"

# ===== ENV =====
BOT_TOKEN        = os.getenv("BOT_TOKEN", os.getenv("TELEGRAM_BOT_TOKEN", "")).strip()
PUBLIC_URL       = os.getenv("PUBLIC_URL", "").strip()
OPENAI_API_KEY   = os.getenv("OPENAI_API_KEY", "").strip()
OPENAI_MODEL     = os.getenv("OPENAI_MODEL", "gpt-4o-mini").strip()
CRYPTOBOT_TOKEN  = os.getenv("CRYPTOBOT_TOKEN", "").strip()
LUMA_API_KEY     = os.getenv("LUMA_API_KEY", "").strip()
RUNWAY_API_KEY   = os.getenv("RUNWAY_API_KEY", "").strip()

# ===== Minimal persistence =====
DB_PATH = os.getenv("BOT_DB", "bot.db")

def db_init() -> None:
    con = sqlite3.connect(DB_PATH)
    cur = con.cursor()
    cur.execute("""
        CREATE TABLE IF NOT EXISTS users (
            user_id INTEGER PRIMARY KEY,
            voice_on INTEGER DEFAULT 0,
            created_at TEXT
        )
    """)
    cur.execute("""
        CREATE TABLE IF NOT EXISTS history (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            user_id INTEGER,
            role TEXT,
            text TEXT,
            ts TEXT
        )
    """)
    con.commit(); con.close()

def db_user_get_or_create(user_id: int) -> Dict[str, Any]:
    con = sqlite3.connect(DB_PATH)
    cur = con.cursor()
    cur.execute("SELECT user_id, voice_on FROM users WHERE user_id=?", (user_id,))
    row = cur.fetchone()
    if not row:
        cur.execute("INSERT INTO users(user_id, voice_on, created_at) VALUES(?,?,?)",
                    (user_id, 0, datetime.now(timezone.utc).isoformat()))
        con.commit()
        row = (user_id, 0)
    con.close()
    return {"user_id": row[0], "voice_on": int(row[1])}

def db_user_set_voice(user_id: int, on: bool) -> None:
    con = sqlite3.connect(DB_PATH)
    cur = con.cursor()
    cur.execute("UPDATE users SET voice_on=? WHERE user_id=?", (1 if on else 0, user_id))
    con.commit(); con.close()

def db_save_turn(user_id: int, role: str, text: str) -> None:
    con = sqlite3.connect(DB_PATH)
    cur = con.cursor()
    cur.execute("INSERT INTO history(user_id, role, text, ts) VALUES(?,?,?,?)",
                (user_id, role, text, datetime.now(timezone.utc).isoformat()))
    con.commit(); con.close()

db_init()

# ===== OpenAI Client Wrapper =====
try:
    from openai import OpenAI
except Exception as e:
    OpenAI = None
    log.error("OpenAI SDK import failed: %s", e)

class OAClient:
    def __init__(self, key: str, model: str):
        if not key or not OpenAI:
            self.client = None
        else:
            self.client = OpenAI(api_key=key)
        self.model = model

    async def chat(self, text: str, sys_prompt: str="You are a helpful assistant.") -> str:
        if not self.client:
            return "OpenAI API key is not configured."
        try:
            res = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role":"system","content": sys_prompt},
                    {"role":"user","content": text}
                ]
            )
            return res.choices[0].message.content.strip()
        except Exception as e:
            log.exception("chat error")
            return f"ÐžÑˆÐ¸Ð±ÐºÐ° OpenAI: {e}"

    async def vision_analyze(self, image_url_or_b64: str, prompt: str="ÐžÐ¿Ð¸ÑˆÐ¸ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ðµ ÐºÑ€Ð°Ñ‚ÐºÐ¾.") -> str:
        if not self.client:
            return "OpenAI API key is not configured."
        content = [{"type":"text","text": prompt}]
        if image_url_or_b64.startswith("http"):
            content.append({"type":"image_url","image_url":{"url": image_url_or_b64}})
        else:
            content.append({"type":"image_url","image_url":{"url": f"data:image/png;base64,{image_url_or_b64}"}})
        try:
            res = self.client.chat.completions.create(
                model=self.model,
                messages=[{"role":"user","content": content}]
            )
            return res.choices[0].message.content.strip()
        except Exception as e:
            log.exception("vision error")
            return f"ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ð¿Ñ€Ð¾Ð°Ð½Ð°Ð»Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ðµ: {e}"

    async def tts(self, text: str, voice: str="alloy", fmt: str="ogg") -> bytes:
        if not self.client:
            raise RuntimeError("OpenAI API key is not configured")
        model = "gpt-4o-mini-tts"
        from tempfile import NamedTemporaryFile
        try:
            with NamedTemporaryFile(delete=False, suffix=f".{fmt}") as tmp:
                tmp_path = tmp.name
            with self.client.audio.speech.with_streaming_response.create(
                model=model, voice=voice, input=text, format=fmt
            ) as resp:
                resp.stream_to_file(tmp_path)
            with open(tmp_path, "rb") as f:
                return f.read()
        except Exception as e:
            logging.warning("Streaming TTS failed: %s", e)
            audio = self.client.audio.speech.create(model=model, voice=voice, input=text, format=fmt)
            try:
                return audio.read()
            except Exception:
                try:
                    b64 = audio.get("audio", {}).get("data")
                    if b64:
                        return base64.b64decode(b64)
                except Exception:
                    pass
                raise

    async def stt(self, audio_bytes: bytes, ext: str="ogg") -> str:
        if not self.client:
            return ""
        import tempfile
        with tempfile.NamedTemporaryFile(delete=False, suffix=f".{ext}") as tmp:
            tmp.write(audio_bytes); tmp_path = tmp.name
        try:
            # Prefer 4o-transcribe if available
            try:
                with open(tmp_path, "rb") as f:
                    r = self.client.audio.transcriptions.create(
                        model="gpt-4o-transcribe", file=f
                    )
                return r.text.strip()
            except Exception:
                with open(tmp_path, "rb") as f:
                    r = self.client.audio.transcriptions.create(
                        model="whisper-1", file=f
                    )
                return r.text.strip()
        finally:
            with contextlib.suppress(Exception):
                os.remove(tmp_path)

    async def image_generate(self, prompt: str, size: str="1024x1024") -> bytes:
        if not self.client:
            raise RuntimeError("OpenAI API key is not configured")
        res = self.client.images.generate(model="gpt-image-1", prompt=prompt, size=size)
        b64 = res.data[0].b64_json
        return base64.b64decode(b64)

    async def image_edit(self, prompt: str, image_png: bytes, mask_png: Optional[bytes], size: str="1024x1024") -> bytes:
        if not self.client:
            raise RuntimeError("OpenAI API key is not configured")
        if mask_png is None:
            res = self.client.images.edits(
                model="gpt-image-1",
                image=[{"image": image_png}],
                prompt=prompt, size=size
            )
        else:
            res = self.client.images.edits(
                model="gpt-image-1",
                image=[{"image": image_png}],
                mask=mask_png,
                prompt=prompt, size=size
            )
        b64 = res.data[0].b64_json
        return base64.b64decode(b64)

# Instantiate
try:
    OPENAI_CLIENT = OAClient(OPENAI_API_KEY, OPENAI_MODEL)
except Exception:
    OPENAI_CLIENT = None

# ===== Image capabilities intent =====
def wants_image_capabilities(text: str) -> bool:
    if not text:
        return False
    low = text.lower()
    img_words = ["Ð¸Ð·Ð¾Ð±Ñ€", "Ñ„Ð¾Ñ‚Ð¾", "ÐºÐ°Ñ€Ñ‚Ð¸Ð½", "Ð¿Ð¸ÐºÑ‡", "image", "photo", "picture"]
    ask_words = ["Ñ‡Ñ‚Ð¾", "Ð¼Ð¾Ð¶", "ÑƒÐ¼Ðµ", "Ð²Ð¾Ð·Ð¼Ð¾Ð¶Ð½", "ÑÐ¿Ð¾ÑÐ¾Ð±ÐµÐ½", "can", "do"]
    return any(w in low for w in img_words) and any(w in low for w in ask_words)

def positive_image_capabilities_text() -> str:
    return (
        "Ð’Ð¾Ñ‚ Ñ‡Ñ‚Ð¾ Ñ ÑƒÐ¼ÐµÑŽ Ñ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸ÑÐ¼Ð¸ Ð¸ Ñ„Ð¾Ñ‚Ð¾:\n"
        "â€¢ ðŸŽ¬ ÐžÐ¶Ð¸Ð²Ð¸Ñ‚ÑŒ Ñ„Ð¾Ñ‚Ð¾ (Imageâ†’Video) Ñ‡ÐµÑ€ÐµÐ· Luma/Runway (Ñ€ÐµÐ°Ð»ÑŒÐ½Ñ‹Ðµ Ð´Ð²Ð¸Ð¶ÐºÐ¸)\n"
        "â€¢ ðŸ§¼ Ð£Ð´Ð°Ð»Ð¸Ñ‚ÑŒ/Ð·Ð°Ð¼ÐµÐ½Ð¸Ñ‚ÑŒ Ñ„Ð¾Ð½ (Ð¿Ñ€Ð¾Ð·Ñ€Ð°Ñ‡Ð½Ñ‹Ð¹ PNG Ð¸Ð»Ð¸ Ñ„Ð¾Ð½ Ð¿Ð¾ Ð¾Ð¿Ð¸ÑÐ°Ð½Ð¸ÑŽ)\n"
        "â€¢ âž•âž– Ð”Ð¾Ð±Ð°Ð²Ð¸Ñ‚ÑŒ/ÑƒÐ´Ð°Ð»Ð¸Ñ‚ÑŒ Ð¾Ð±ÑŠÐµÐºÑ‚ Ð¸Ð»Ð¸ Ñ‡ÐµÐ»Ð¾Ð²ÐµÐºÐ°\n"
        "â€¢ ðŸ§© Ð Ð°ÑÑˆÐ¸Ñ€Ð¸Ñ‚ÑŒ ÐºÐ°Ð´Ñ€ (Outpaint) â€” Ð´Ð¾Ñ€Ð¸ÑÐ¾Ð²Ð°Ñ‚ÑŒ Ð½ÐµÐ²Ð¸Ð´Ð¸Ð¼Ñ‹Ðµ ÐºÑ€Ð°Ñ/Ñ€Ð°ÐºÑƒÑ€ÑÑ‹\n"
        "â€¢ ðŸŽ¥ ÐŸÐ¾Ð²ÐµÑ€Ð½ÑƒÑ‚ÑŒ ÐºÐ°Ð¼ÐµÑ€Ñƒ (Ð¾Ñ€Ð±Ð¸Ñ‚/Ð¿Ð°Ð½/Ñ‚Ð¸Ð»Ñ‚) Ð¸ Ð»Ñ‘Ð³ÐºÐ°Ñ Ð´Ð¸Ð½Ð°Ð¼Ð¸ÐºÐ°\n"
        "â€¢ ðŸ“ Storyboard â€” ÑÑ†ÐµÐ½Ð°Ñ€Ð¸Ð¹ Â«Ð³Ð»Ð¾Ð±Ð°Ð»ÑŒÐ½Ð¾Ð³Ð¾ Ð¾Ð¶Ð¸Ð²Ð»ÐµÐ½Ð¸ÑÂ» Ñ Ð´Ð²Ð¸Ð¶ÐµÐ½Ð¸ÐµÐ¼ Ð»ÑŽÐ´ÐµÐ¹ Ð¸ Ð¾Ð±ÑŠÐµÐºÑ‚Ð¾Ð²\n\n"
        "Ð­Ñ‚Ð¾ Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÐµÑ‚ Ð¸ Ð¿Ð¾ Ñ‚ÐµÐºÑÑ‚Ñƒ, Ð¸ Ð¿Ð¾ Ð³Ð¾Ð»Ð¾ÑÑƒ. ÐŸÑ€Ð¸ÑˆÐ»Ð¸Ñ‚Ðµ Ñ„Ð¾Ñ‚Ð¾ â€” Ð¿Ð¾ÐºÐ°Ð¶Ñƒ ÐºÐ½Ð¾Ð¿ÐºÐ¸ Ð±Ñ‹ÑÑ‚Ñ€Ñ‹Ñ… Ð´ÐµÐ¹ÑÑ‚Ð²Ð¸Ð¹."
    )

# ===== Utils =====
def build_outpaint_inputs(base_png: bytes, expand_pct: float = 0.25) -> tuple[bytes, bytes]:
    base = Image.open(io.BytesIO(base_png)).convert("RGBA")
    w, h = base.size
    dx, dy = int(w * expand_pct), int(h * expand_pct)
    canvas = Image.new("RGBA", (w + 2*dx, h + 2*dy), (0,0,0,0))
    canvas.paste(base, (dx, dy))
    mask = Image.new("L", canvas.size, 255)
    draw = ImageDraw.Draw(mask)
    draw.rectangle((dx, dy, dx+w, dy+h), fill=0)
    b_img = io.BytesIO(); canvas.save(b_img, format="PNG")
    b_mask = io.BytesIO(); mask.save(b_mask, format="PNG")
    return b_img.getvalue(), b_mask.getvalue()

def human_exc(e: Exception) -> str:
    s = str(e)
    return s if len(s) < 400 else s[:400] + "â€¦"

# ===== Telegram Handlers =====
START_TEXT = (
    "ÐŸÑ€Ð¸Ð²ÐµÑ‚! Ð­Ñ‚Ð¾ BOT GPTâ€‘5, Runway, Midjourney, Luma, Deepgram.\n\n"
    "Ð§Ñ‚Ð¾ ÑƒÐ¼ÐµÑŽ:\n"
    "â€¢ GPTâ€‘5 Ñ‚ÐµÐºÑÑ‚Ñ‹, ÐºÐ¾Ð´, Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ñ‹\n"
    "â€¢ Midjourney â€” Ñ„Ð¾Ñ‚Ð¾Ñ€ÐµÐ°Ð»Ð¸ÑÑ‚Ð¸Ñ‡Ð½Ñ‹Ðµ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ñ\n"
    "â€¢ Luma/Runway â€” Ð²Ð¸Ð´ÐµÐ¾ Ð¸Ð· Ñ„Ð¾Ñ‚Ð¾ (imageâ†’video)\n"
    "â€¢ Deepgram/OpenAI â€” Ñ€ÐµÑ‡ÑŒâ†”Ñ‚ÐµÐºÑÑ‚\n\n"
    "ÐŸÐ¾Ð´ÑÐºÐ°Ð·ÐºÐ¸:\n"
    "â€¢ /img ÐºÐ¾Ñ‚ Ð² Ð¾Ñ‡ÐºÐ°Ñ… â€” ÑÐ³ÐµÐ½ÐµÑ€Ð¸Ñ€ÑƒÐµÑ‚ ÐºÐ°Ñ€Ñ‚Ð¸Ð½ÐºÑƒ\n"
    "â€¢ Â«ÐžÐ¶Ð¸Ð²Ð¸ Ñ„Ð¾Ñ‚Ð¾â€¦ 9 ÑÐµÐº 9:16Â» â€” Luma/Runway (ÐµÑÐ»Ð¸ Ð¿Ð¾Ð´ÐºÐ»ÑŽÑ‡ÐµÐ½Ñ‹ ÐºÐ»ÑŽÑ‡Ð¸)\n"
    "â€¢ /voice_on Ð¸ /voice_off â€” Ð¾Ð·Ð²ÑƒÑ‡ÐºÐ° Ð¾Ñ‚Ð²ÐµÑ‚Ð¾Ð²."
)

EXAMPLES_TEXT = (
    "ÐŸÑ€Ð¸Ð¼ÐµÑ€Ñ‹:\n"
    "â€¢ ÐžÐ¶Ð¸Ð²Ð¸ Ñ„Ð¾Ñ‚Ð¾ Ð² ÑÑ‚Ð¸Ð»Ðµ ÐºÐ¸Ð½Ð¾, 6 ÑÐµÐºÑƒÐ½Ð´ 9:16 â€” Luma/Runway\n"
    "â€¢ Ð£Ð´Ð°Ð»Ð¸ Ñ„Ð¾Ð½ Ñƒ ÑÑ‚Ð¾Ð¹ ÐºÐ°Ñ€Ñ‚Ð¸Ð½ÐºÐ¸\n"
    "â€¢ Ð”Ð¾Ñ€Ð¸ÑÑƒÐ¹ ÑÐ¿Ñ€Ð°Ð²Ð° Ñ‚ÐµÑ€Ñ€Ð°ÑÑƒ Ð¸ Ð¼Ð¾Ñ€Ðµ (outpaint)\n"
    "â€¢ ÐŸÐ¾Ð²ÐµÑ€Ð½Ð¸ ÐºÐ°Ð¼ÐµÑ€Ñƒ Ð²Ð¾ÐºÑ€ÑƒÐ³ Ð½Ð° 20 Ð³Ñ€Ð°Ð´ÑƒÑÐ¾Ð²\n"
)

MODES_TEXT = (
    "Ð”Ð²Ð¸Ð¶ÐºÐ¸: GPT / Luma / Runway / Images / Docs.\n"
    "ÐžÐ·Ð²ÑƒÑ‡ÐºÐ°: /voice_on, /voice_off."
)

async def cmd_start(update: Update, context: ContextTypes.DEFAULT_TYPE):
    user = update.effective_user
    db_user_get_or_create(user.id)
    kb = ReplyKeyboardMarkup([[KeyboardButton("/modes")],[KeyboardButton("/plans")]], resize_keyboard=True)
    await update.effective_message.reply_text(START_TEXT, reply_markup=kb)

async def cmd_help(update: Update, context: ContextTypes.DEFAULT_TYPE):
    await update.effective_message.reply_text("Ð—Ð°Ð´Ð°Ð¹ Ð²Ð¾Ð¿Ñ€Ð¾Ñ Ð¸Ð»Ð¸ Ð¿Ñ€Ð¸ÑˆÐ»Ð¸ Ñ„Ð¾Ñ‚Ð¾/Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚. /examples â€” Ð¿Ñ€Ð¸Ð¼ÐµÑ€Ñ‹.")

async def cmd_examples(update: Update, context: ContextTypes.DEFAULT_TYPE):
    await update.effective_message.reply_text(EXAMPLES_TEXT)

async def cmd_modes(update: Update, context: ContextTypes.DEFAULT_TYPE):
    await update.effective_message.reply_text(MODES_TEXT)

async def cmd_ver(update: Update, context: ContextTypes.DEFAULT_TYPE):
    await update.effective_message.reply_text(f"Ð’ÐµÑ€ÑÐ¸Ñ: {VERSION_TAG}")

async def cmd_voice_on(update: Update, context: ContextTypes.DEFAULT_TYPE):
    db_user_set_voice(update.effective_user.id, True)
    await update.effective_message.reply_text("ðŸ”Š ÐžÐ·Ð²ÑƒÑ‡ÐºÐ° Ð²ÐºÐ»ÑŽÑ‡ÐµÐ½Ð°.")

async def cmd_voice_off(update: Update, context: ContextTypes.DEFAULT_TYPE):
    db_user_set_voice(update.effective_user.id, False)
    await update.effective_message.reply_text("ðŸ”‡ ÐžÐ·Ð²ÑƒÑ‡ÐºÐ° Ð²Ñ‹ÐºÐ»ÑŽÑ‡ÐµÐ½Ð°.")

# ===== CryptoBot =====
async def create_cryptobot_invoice(amount: float = 5.0, asset: str="USDT", desc: str="GPT5 PRO Subscription") -> Optional[str]:
    if not CRYPTOBOT_TOKEN:
        return None
    headers = {"Crypto-Pay-API-Token": CRYPTOBOT_TOKEN, "Content-Type":"application/json"}
    payload = {"asset": asset, "amount": str(amount), "description": desc}
    try:
        async with httpx.AsyncClient(timeout=20) as cli:
            r = await cli.post("https://pay.crypt.bot/api/createInvoice", headers=headers, json=payload)
            r.raise_for_status()
            data = r.json()
            if data.get("ok"):
                return data["result"]["pay_url"]
    except Exception as e:
        log.error("cryptobot createInvoice error: %s", e)
    return None

async def cmd_plans(update: Update, context: ContextTypes.DEFAULT_TYPE):
    url = await create_cryptobot_invoice()
    if url:
        await update.effective_message.reply_text(f"ÐžÐ¿Ð»Ð°Ñ‚Ð° Ð¿Ð¾Ð´Ð¿Ð¸ÑÐºÐ¸: {url}")
    else:
        await update.effective_message.reply_text("CryptoBot Ð½Ðµ Ð½Ð°ÑÑ‚Ñ€Ð¾ÐµÐ½. Ð£ÐºÐ°Ð¶Ð¸ CRYPTOBOT_TOKEN Ð² Ð¿ÐµÑ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ñ… Ð¾ÐºÑ€ÑƒÐ¶ÐµÐ½Ð¸Ñ.")

# ===== /img generate =====
async def cmd_img(update: Update, context: ContextTypes.DEFAULT_TYPE):
    prompt = " ".join(context.args) if context.args else "a cute cat with glasses, studio photo"
    try:
        png = await OPENAI_CLIENT.image_generate(prompt, size="1024x1024")
        await update.effective_message.reply_photo(InputFile(io.BytesIO(png), filename="image.png"), caption="Ð“Ð¾Ñ‚Ð¾Ð²Ð¾.")
    except Exception as e:
        await update.effective_message.reply_text(f"ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ ÑÐ³ÐµÐ½ÐµÑ€Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ: {human_exc(e)}")

# ===== Text handler =====
async def on_text(update: Update, context: ContextTypes.DEFAULT_TYPE):
    user = update.effective_user
    text = update.effective_message.text or ""
    if wants_image_capabilities(text):
        await update.effective_message.reply_text(positive_image_capabilities_text())
        return

    # Outpaint follow-up
    await_outp = context.user_data.pop("await_outpaint", None)
    if await_outp:
        image_id = await_outp.get("image_id")
        meta = context.user_data.get("images_cache", {}).get(image_id)
        if not meta:
            await update.effective_message.reply_text("ÐÐµ Ð½Ð°ÑˆÑ‘Ð» Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ðµ. ÐŸÑ€Ð¸ÑˆÐ»Ð¸Ñ‚Ðµ ÑÐ½Ð¾Ð²Ð°.")
            return
        await update.effective_message.reply_text("Ð”Ð¾Ñ€Ð¸ÑÐ¾Ð²Ñ‹Ð²Ð°ÑŽ ÐºÑ€Ð°Ñ ÐºÐ°Ð´Ñ€Ð°â€¦")
        try:
            file = await context.bot.get_file(meta["file_id"])
            base_bytes = bytes(await file.download_as_bytearray())
            expanded_png, mask_png = build_outpaint_inputs(base_bytes, expand_pct=0.25)
            edited = await OPENAI_CLIENT.image_edit(text or "extend the scene naturally", expanded_png, mask_png, size="1024x1024")
            await update.effective_message.reply_document(InputFile(io.BytesIO(edited), filename="outpaint.png"),
                                                         caption="Ð“Ð¾Ñ‚Ð¾Ð²Ð¾: Ñ€Ð°ÑÑˆÐ¸Ñ€Ð¸Ð» ÐºÐ°Ð´Ñ€ (Outpaint).")
        except Exception as e:
            await update.effective_message.reply_text(f"ÐžÑˆÐ¸Ð±ÐºÐ° outpaint: {human_exc(e)}")
        return

    db_save_turn(user.id, "user", text)
    reply = await OPENAI_CLIENT.chat(text, sys_prompt="Ð‘ÑƒÐ´ÑŒ ÐºÑ€Ð°Ñ‚ÐºÐ¸Ð¼ Ð¸ Ð¿Ð¾Ð»ÐµÐ·Ð½Ñ‹Ð¼.")
    db_save_turn(user.id, "assistant", reply)
    await update.effective_message.reply_text(reply)

    # TTS if on
    try:
        info = db_user_get_or_create(user.id)
        if info["voice_on"]:
            ogg = await OPENAI_CLIENT.tts(reply, fmt="ogg")
            try:
                await update.effective_message.reply_voice(ogg, caption="")
            except Exception:
                mp3 = await OPENAI_CLIENT.tts(reply, fmt="mp3")
                await update.effective_message.reply_audio(mp3, caption="")
    except Exception as e:
        await update.effective_message.reply_text(f"ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ð¾Ð·Ð²ÑƒÑ‡Ð¸Ñ‚ÑŒ: {human_exc(e)}")

# ===== Voice handler =====
async def on_voice(update: Update, context: ContextTypes.DEFAULT_TYPE):
    v = update.effective_message.voice or update.effective_message.audio
    if not v:
        return
    file = await context.bot.get_file(v.file_id)
    data = await file.download_as_bytearray()
    text = await OPENAI_CLIENT.stt(bytes(data), ext="ogg")
    await update.effective_message.reply_text(f"ðŸ—£ {text}")
    if wants_image_capabilities(text):
        await update.effective_message.reply_text(positive_image_capabilities_text())
        return
    db_save_turn(update.effective_user.id, "user", text)
    reply = await OPENAI_CLIENT.chat(text)
    db_save_turn(update.effective_user.id, "assistant", reply)
    await update.effective_message.reply_text(reply)
    # TTS optional
    try:
        info = db_user_get_or_create(update.effective_user.id)
        if info["voice_on"]:
            ogg = await OPENAI_CLIENT.tts(reply, fmt="ogg")
            try:
                await update.effective_message.reply_voice(ogg)
            except Exception:
                mp3 = await OPENAI_CLIENT.tts(reply, fmt="mp3")
                await update.effective_message.reply_audio(mp3)
    except Exception:
        pass

# ===== Photo handler =====
from rembg import remove as rembg_remove

def kb_photo_actions(image_id: str) -> InlineKeyboardMarkup:
    return InlineKeyboardMarkup([
        [InlineKeyboardButton("ðŸŽ¬ ÐžÐ¶Ð¸Ð²Ð¸Ñ‚ÑŒ Ñ„Ð¾Ñ‚Ð¾ (Imageâ†’Video)", callback_data=f"anim:{image_id}")],
        [InlineKeyboardButton("ðŸ§¼ Ð£Ð´Ð°Ð»Ð¸Ñ‚ÑŒ Ñ„Ð¾Ð½", callback_data=f"rmbg:{image_id}"),
         InlineKeyboardButton("ðŸž Ð—Ð°Ð¼ÐµÐ½Ð¸Ñ‚ÑŒ Ñ„Ð¾Ð½", callback_data=f"bg:{image_id}")],
        [InlineKeyboardButton("âž•âž– ÐžÐ±ÑŠÐµÐºÑ‚Ñ‹/Ð»ÑŽÐ´Ð¸", callback_data=f"obj:{image_id}"),
         InlineKeyboardButton("ðŸ§© Outpaint", callback_data=f"outp:{image_id}")],
        [InlineKeyboardButton("ðŸŽ¥ ÐŸÐ¾Ð²ÐµÑ€Ð½ÑƒÑ‚ÑŒ ÐºÐ°Ð¼ÐµÑ€Ñƒ", callback_data=f"cam:{image_id}"),
         InlineKeyboardButton("ðŸ“ Storyboard", callback_data=f"story:{image_id}")],
    ])

async def on_photo(update: Update, context: ContextTypes.DEFAULT_TYPE):
    msg = update.effective_message
    caption = msg.caption or ""
    photos = msg.photo
    if not photos:
        return
    photo = photos[-1]  # best quality
    image_id = photo.file_unique_id
    # Cache
    cache = context.user_data.setdefault("images_cache", {})
    cache[image_id] = {"file_id": photo.file_id, "caption": caption}

    await msg.reply_text("Ð¤Ð¾Ñ‚Ð¾ Ð¿Ð¾Ð»ÑƒÑ‡ÐµÐ½Ð¾. Ð’Ñ‹Ð±ÐµÑ€Ð¸Ñ‚Ðµ Ð´ÐµÐ¹ÑÑ‚Ð²Ð¸Ðµ:", reply_markup=kb_photo_actions(image_id))
    await msg.reply_text(positive_image_capabilities_text())

    # Auto reaction to caption
    if caption:
        try:
            f = await context.bot.get_file(photo.file_id)
            url = f.file_path  # Telegram CDN URL
            ans = await OPENAI_CLIENT.vision_analyze(url, f"ÐžÑ‚Ñ€ÐµÐ°Ð³Ð¸Ñ€ÑƒÐ¹ Ð´Ñ€ÑƒÐ¶ÐµÐ»ÑŽÐ±Ð½Ð¾ Ð½Ð° Ð¿Ð¾Ð´Ð¿Ð¸ÑÑŒ Ð¸ Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶Ð¸ ÑƒÐ»ÑƒÑ‡ÑˆÐµÐ½Ð¸Ñ. ÐŸÐ¾Ð´Ð¿Ð¸ÑÑŒ: Â«{caption}Â».")
            await msg.reply_text(ans)
        except Exception:
            pass

# ===== Callbacks =====
async def on_cb(update: Update, context: ContextTypes.DEFAULT_TYPE):
    q = update.callback_query
    await q.answer()
    data = q.data or ""
    user_data = context.user_data
    cache = user_data.get("images_cache", {})

    async def load_image_bytes(image_id: str) -> bytes:
        meta = cache.get(image_id)
        if not meta:
            raise RuntimeError("ÐÐµ Ð½Ð°Ð¹Ð´ÐµÐ½Ð¾ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ðµ Ð² ÐºÑÑˆÐµ.")
        file = await context.bot.get_file(meta["file_id"])
        return bytes(await file.download_as_bytearray())

    if data.startswith("rmbg:"):
        img_id = data.split(":",1)[1]
        try:
            raw = await load_image_bytes(img_id)
            out = rembg_remove(raw)
            await q.message.reply_document(InputFile(io.BytesIO(out), filename="no-bg.png"), caption="Ð“Ð¾Ñ‚Ð¾Ð²Ð¾: Ñ„Ð¾Ð½ ÑƒÐ´Ð°Ð»Ñ‘Ð½ (PNG).")
        except Exception as e:
            await q.message.reply_text(f"ÐžÑˆÐ¸Ð±ÐºÐ° ÑƒÐ´Ð°Ð»ÐµÐ½Ð¸Ñ Ñ„Ð¾Ð½Ð°: {human_exc(e)}")
        return

    if data.startswith("bg:"):
        img_id = data.split(":",1)[1]
        user_data["await_bg_replace"] = {"image_id": img_id}
        await q.message.reply_text("ÐÐ°Ð¿Ð¸ÑˆÐ¸, ÐºÐ°ÐºÐ¾Ð¹ Ñ„Ð¾Ð½ ÑÐ¾Ð·Ð´Ð°Ñ‚ÑŒ (Ð¾Ð¿Ð¸ÑÐ°Ð½Ð¸Ðµ ÑÑ†ÐµÐ½Ñ‹/ÑÑ‚Ð¸Ð»Ñ).")
        return

    if data.startswith("outp:"):
        img_id = data.split(":",1)[1]
        user_data["await_outpaint"] = {"image_id": img_id}
        await q.message.reply_text("ÐžÐ¿Ð¸ÑˆÐ¸, Ñ‡Ñ‚Ð¾ Ð´Ð¾Ñ€Ð¸ÑÐ¾Ð²Ð°Ñ‚ÑŒ Ð²Ð¾ÐºÑ€ÑƒÐ³ ÐºÐ°Ð´Ñ€Ð° (Ñ„Ð¾Ð½/Ð¸Ð½Ñ‚ÐµÑ€ÑŒÐµÑ€/ÑƒÐ»Ð¸Ñ†Ñƒ).")
        return

    if data.startswith("obj:"):
        await q.message.reply_text("ÐžÐ¿Ð¸ÑˆÐ¸, Ñ‡Ñ‚Ð¾ Ð´Ð¾Ð±Ð°Ð²Ð¸Ñ‚ÑŒ Ð¸Ð»Ð¸ Ñ‡Ñ‚Ð¾/Ð³Ð´Ðµ ÑƒÐ´Ð°Ð»Ð¸Ñ‚ÑŒ. (ÐŸÑ€Ð¸Ð¼ÐµÑ‡Ð°Ð½Ð¸Ðµ: Ñ‚Ð¾Ð½ÐºÐ°Ñ Ð¼Ð°ÑÐºÐ° Ð¿Ð¾Ñ‚Ñ€ÐµÐ±ÑƒÐµÑ‚ ÑƒÑ‚Ð¾Ñ‡Ð½ÐµÐ½Ð¸Ð¹).")
        user_data["await_obj_edit"] = {"note": "text-guided edit"}
        return

    if data.startswith("story:"):
        try:
            story_prompt = "Ð¡Ð´ÐµÐ»Ð°Ð¹ ÐºÐ¾Ñ€Ð¾Ñ‚ÐºÐ¸Ð¹ storyboard Ð¾Ð¶Ð¸Ð²Ð»ÐµÐ½Ð¸Ñ ÐºÐ°Ð´Ñ€Ð° (3â€“6 ÑÑ†ÐµÐ½) Ñ Ð´Ð²Ð¸Ð¶ÐµÐ½Ð¸ÑÐ¼Ð¸ Ñ‡ÐµÐ»Ð¾Ð²ÐµÐºÐ° Ð¸ Ð¾Ð±ÑŠÐµÐºÑ‚Ð¾Ð², ÐºÑ€Ð°Ñ‚ÐºÐ¾."
            res = await OPENAI_CLIENT.chat(story_prompt)
            await q.message.reply_text(res)
        except Exception as e:
            await q.message.reply_text(f"ÐžÑˆÐ¸Ð±ÐºÐ° storyboard: {human_exc(e)}")
        return

    if data.startswith("anim:"):
        if not (LUMA_API_KEY or RUNWAY_API_KEY):
            await q.message.reply_text("Ð”Ð»Ñ Ð¾Ð¶Ð¸Ð²Ð»ÐµÐ½Ð¸Ñ Ñ„Ð¾Ñ‚Ð¾ Ð¿Ð¾Ð´ÐºÐ»ÑŽÑ‡Ð¸ LUMA_API_KEY Ð¸Ð»Ð¸ RUNWAY_API_KEY Ð² Ð¿ÐµÑ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ñ… Ð¾ÐºÑ€ÑƒÐ¶ÐµÐ½Ð¸Ñ.")
            return
        await q.message.reply_text("Ð¡Ð¾Ð·Ð´Ð°ÑŽ Ð·Ð°Ð´Ð°Ñ‡Ñƒ Ð¾Ð¶Ð¸Ð²Ð»ÐµÐ½Ð¸Ñ (imageâ†’video)â€¦ (Ð·Ð°Ð³Ð»ÑƒÑˆÐºÐ°).")
        return

    if data.startswith("cam:"):
        if not (LUMA_API_KEY or RUNWAY_API_KEY):
            await q.message.reply_text("Ð”Ð»Ñ Ð¿Ð¾Ð²Ð¾Ñ€Ð¾Ñ‚Ð° ÐºÐ°Ð¼ÐµÑ€Ñ‹ Ð¿Ð¾Ð´ÐºÐ»ÑŽÑ‡Ð¸ LUMA_API_KEY Ð¸Ð»Ð¸ RUNWAY_API_KEY.")
            return
        await q.message.reply_text("Ð¡Ð¾Ð·Ð´Ð°ÑŽ Ð·Ð°Ð´Ð°Ñ‡Ñƒ Ð½Ð° Ð¿Ð¾Ð²Ð¾Ñ€Ð¾Ñ‚ ÐºÐ°Ð¼ÐµÑ€Ñ‹â€¦ (Ð·Ð°Ð³Ð»ÑƒÑˆÐºÐ°).")
        return

# ===== Background actions from text follow-up =====
async def on_text_followups(update: Update, context: ContextTypes.DEFAULT_TYPE) -> bool:
    msg = update.effective_message
    text = msg.text or ""
    user_data = context.user_data
    cache = user_data.get("images_cache", {})

    # Replace background
    await_bg = user_data.pop("await_bg_replace", None)
    if await_bg:
        try:
            meta = cache.get(await_bg["image_id"])
            if not meta:
                await msg.reply_text("ÐÐµ Ð½Ð°ÑˆÑ‘Ð» Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ðµ. ÐŸÑ€Ð¸ÑˆÐ»Ð¸Ñ‚Ðµ ÑÐ½Ð¾Ð²Ð°.")
                return True
            file = await context.bot.get_file(meta["file_id"])
            base_bytes = bytes(await file.download_as_bytearray())
            # 1) Remove to alpha
            from rembg import remove as rembg_remove_local
            cut = rembg_remove_local(base_bytes)
            fg = Image.open(io.BytesIO(cut)).convert("RGBA")
            # 2) Generate background
            bg_png = await OPENAI_CLIENT.image_generate(text or "studio background", size="1024x1024")
            bg = Image.open(io.BytesIO(bg_png)).convert("RGBA").resize(fg.size)
            # 3) Composite
            canvas = Image.new("RGBA", fg.size, (0,0,0,0))
            canvas.paste(bg, (0,0))
            canvas.alpha_composite(fg)
            out = io.BytesIO(); canvas.save(out, format="PNG")
            await msg.reply_document(InputFile(io.BytesIO(out.getvalue()), filename="rebackground.png"),
                                     caption="Ð“Ð¾Ñ‚Ð¾Ð²Ð¾: Ñ„Ð¾Ð½ Ð·Ð°Ð¼ÐµÐ½Ñ‘Ð½.")
        except Exception as e:
            await msg.reply_text(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð·Ð°Ð¼ÐµÐ½Ñ‹ Ñ„Ð¾Ð½Ð°: {human_exc(e)}")
        return True

    # Object edits (simple text-to-edit without precise mask)
    await_obj = user_data.pop("await_obj_edit", None)
    if await_obj:
        try:
            meta = None
            if cache:
                meta = list(cache.values())[-1]
            if not meta:
                await msg.reply_text("ÐÐµ Ð½Ð°ÑˆÑ‘Ð» Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ðµ. ÐŸÑ€Ð¸ÑˆÐ»Ð¸Ñ‚Ðµ ÑÐ½Ð¾Ð²Ð°.")
                return True
            file = await context.bot.get_file(meta["file_id"])
            base_bytes = bytes(await file.download_as_bytearray())
            edited = await OPENAI_CLIENT.image_edit(text or "enhance", base_bytes, None, size="1024x1024")
            await msg.reply_document(InputFile(io.BytesIO(edited), filename="edit.png"), caption="Ð“Ð¾Ñ‚Ð¾Ð²Ð¾.")
        except Exception as e:
            await msg.reply_text(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ñ€ÐµÐ´Ð°ÐºÑ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ: {human_exc(e)}")
        return True

    return False

# ===== Router: wrap text updates to check followups first =====
async def text_router(update: Update, context: ContextTypes.DEFAULT_TYPE):
    handled = await on_text_followups(update, context)
    if handled:
        return
    await on_text(update, context)

# ===== App =====
def build_app() -> Application:
    if not BOT_TOKEN:
        log.error("BOT_TOKEN is not set")
        sys.exit(1)
    app = Application.builder().token(BOT_TOKEN).build()

    app.add_handler(CommandHandler("start", cmd_start))
    app.add_handler(CommandHandler("help", cmd_help))
    app.add_handler(CommandHandler("examples", cmd_examples))
    app.add_handler(CommandHandler("modes", cmd_modes))
    app.add_handler(CommandHandler("plans", cmd_plans))
    app.add_handler(CommandHandler("voice_on", cmd_voice_on))
    app.add_handler(CommandHandler("voice_off", cmd_voice_off))
    app.add_handler(CommandHandler("img", cmd_img))
    app.add_handler(CommandHandler("ver", cmd_ver))

    app.add_handler(CallbackQueryHandler(on_cb))
    app.add_handler(MessageHandler(filters.PHOTO, on_photo))
    app.add_handler(MessageHandler(filters.VOICE | filters.AUDIO, on_voice))
    app.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, text_router))

    return app

async def main_async():
    app = build_app()
    await app.initialize()
    await app.start()
    log.info("Bot started: %s", VERSION_TAG)
    await app.updater.start_polling(allowed_updates=Update.ALL_TYPES)
    try:
        while True:
            await asyncio.sleep(3600)
    except asyncio.CancelledError:
        pass
    finally:
        await app.updater.stop()
        await app.stop()

def main():
    try:
        asyncio.run(main_async())
    except (KeyboardInterrupt, SystemExit):
        pass

if __name__ == "__main__":
    main()
